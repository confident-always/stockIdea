#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­é—´å±‚ç”»çº¿è„šæœ¬ - æ•´åˆåŸºç¡€å±‚åŠŸèƒ½å¹¶æ·»åŠ AnchorMçº¿
åŒ…å«å®Œæ•´çš„åŸºç¡€ç”»çº¿åŠŸèƒ½ + AnchorMçº¿åˆ†æ

åŠŸèƒ½ç‰¹æ€§ï¼š
1. ä»resByFilterä¸­æå–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
2. æ™ºèƒ½æ•°æ®éªŒè¯å’Œæ¸…æ´—
3. ZigZagé˜¶æ®µä½ç‚¹æ£€æµ‹
4. é«˜è´¨é‡Kçº¿å›¾è¡¨ç»˜åˆ¶ï¼ˆçº¢æ¶¨ç»¿è·Œï¼‰
5. AnchorMçº¿åŠ¨æ€ä¼˜åŒ–å’Œç»˜åˆ¶
6. å¤šçº¿ç¨‹æ‰¹é‡å¤„ç†
7. å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

ä½¿ç”¨æ–¹æ³•ï¼š
    # å¤„ç†æŒ‡å®šæ—¥æœŸçš„è‚¡ç¥¨
    python draw_lines_mid.py --date 2025-10-20
    
    # æŒ‡å®šçº¿ç¨‹æ•°
    python draw_lines_mid.py --date 2025-10-20 --workers 4
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging
from datetime import datetime
import glob
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time

# è®¾ç½®matplotlibåç«¯å’Œå­—ä½“
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯ï¼Œæ”¯æŒå¤šçº¿ç¨‹
import matplotlib.pyplot as plt

# å­—ä½“é…ç½® - ä½¿ç”¨macOSç³»ç»Ÿæ”¯æŒçš„ä¸­æ–‡å­—ä½“
plt.rcParams['font.family'] = ['Heiti TC', 'PingFang HK', 'Arial Unicode MS', 'Arial', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] [%(threadName)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('draw_lines_mid.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# çº¿ç¨‹é”
progress_lock = threading.Lock()
matplotlib_lock = threading.Lock()


class MidLineDrawer:
    """ä¸­é—´å±‚ç”»çº¿å™¨ - æ•´åˆåŸºç¡€å±‚åŠŸèƒ½å¹¶æ·»åŠ AnchorMçº¿"""
    
    def __init__(self, config_file: str = "lineConfig.json"):
        """åˆå§‹åŒ–ä¸­é—´å±‚ç”»çº¿å™¨"""
        self.config_file = config_file
        self.percent_list, self.anchor_m_config = self._load_config()
        self.stock_info = self._load_stock_info()
        self.processed_count = 0
        self.total_count = 0
        logger.info(f"âœ… ä¸­é—´å±‚ç”»çº¿å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š åŠ è½½{len(self.percent_list)}ä¸ªç™¾åˆ†æ¯”é…ç½®: {self.percent_list}")
        logger.info(f"ğŸ“ˆ åŠ è½½{len(self.stock_info)}åªè‚¡ç¥¨ä¿¡æ¯")
        logger.info(f"ğŸ”§ AnchorMåŠŸèƒ½: {'å¯ç”¨' if self.anchor_m_config.get('enabled', True) else 'ç¦ç”¨'}")
    
    def _load_config(self) -> Tuple[List[str], Dict]:
        """åŠ è½½é…ç½®æ–‡ä»¶ä¸­çš„ç™¾åˆ†æ¯”æ•°æ®ã€ZigZagå‚æ•°å’ŒAnchorMé…ç½®"""
        try:
            config_path = Path(self.config_file)
            if not config_path.exists():
                config_path = Path("..") / self.config_file
                if not config_path.exists():
                    logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                    default_percents = ["3%", "16%", "25%", "34%", "50%", "67%", "128%", "228%", "247%", "323%", "457%", "589%", "636%", "770%", "823%", "935%"]
                    self.zigzag_period = 49
                    default_anchor_m = {
                        'enabled': True,
                        'zigzag_percent': 10,
                        'pivot_window': 3,
                        'm_range': {'start': 13.0, 'end': 9.0, 'step': -0.1},
                        'max_k': 20,
                        'match_tolerance_ratio': 0.006,
                        'min_matches': 3,
                        'tiebreaker_prefer_higher_M': True,
                        'line_style': {'color': '#8A2BE2', 'linewidth': 3.0, 'alpha': 0.9},
                        'text_style': {'fontsize': 14},
                        'annotate_format': 'K={K} ä»·æ ¼={price}'
                    }
                    return default_percents, default_anchor_m
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                percent_dic = config.get('percent_dic', [])
                self.zigzag_period = config.get('zigzag_period', 49)
                anchor_m_config = config.get('anchorMLines', {})
                
                if not anchor_m_config:
                    anchor_m_config = {'enabled': False}
                
                logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                logger.info(f"ğŸ”§ ZigZagå‘¨æœŸ: {self.zigzag_period}%")
                return percent_dic, anchor_m_config
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            default_percents = ["3%", "16%", "25%", "34%", "50%", "67%", "128%", "228%", "247%", "323%", "457%", "589%", "636%", "770%", "823%", "935%"]
            self.zigzag_period = 49
            default_anchor_m = {'enabled': False}
            return default_percents, default_anchor_m
    
    def _load_stock_info(self) -> Dict[str, Dict[str, str]]:
        """ä»stocklist.csvåŠ è½½è‚¡ç¥¨ä¿¡æ¯ï¼ˆåç§°ã€è¡Œä¸šã€å¸‚ç›ˆç‡ã€æ€»è‚¡æœ¬ï¼‰"""
        stock_info = {}
        try:
            possible_paths = ["../stocklist.csv", "stocklist.csv", "./stocklist.csv"]
            stocklist_file = None
            
            for path in possible_paths:
                if os.path.exists(path):
                    stocklist_file = path
                    break
            
            if stocklist_file:
                try:
                    df = pd.read_csv(stocklist_file)
                    logger.info(f"ğŸ“ ä»{stocklist_file}åŠ è½½è‚¡ç¥¨ä¿¡æ¯")
                    
                    required_cols = ['symbol', 'name', 'industry', 'pe', 'total_share']
                    if all(col in df.columns for col in required_cols):
                        for _, row in df.iterrows():
                            code = str(row['symbol']).zfill(6)
                            name = str(row['name'])
                            industry = str(row['industry']) if pd.notna(row['industry']) else "æœªçŸ¥è¡Œä¸š"
                            pe = row['pe'] if pd.notna(row['pe']) else 0
                            total_share = row['total_share'] if pd.notna(row['total_share']) else 0
                            
                            stock_info[code] = {
                                'name': name, 
                                'industry': industry,
                                'pe': pe,
                                'total_share': total_share  # æ€»è‚¡æœ¬ï¼ˆäº¿è‚¡ï¼‰ï¼Œç”¨äºè®¡ç®—æ€»å¸‚å€¼
                            }
                        
                        logger.info(f"âœ… ä»stocklist.csvåŠ è½½è‚¡ç¥¨ä¿¡æ¯å®Œæˆï¼Œå…±{len(stock_info)}åªè‚¡ç¥¨")
                        return stock_info
                    else:
                        logger.warning(f"âš ï¸ stocklist.csvç¼ºå°‘å¿…è¦åˆ—")
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–stocklist.csvå¤±è´¥: {e}")
            
            logger.info("ğŸ“ stocklist.csvä¸å¯ç”¨")
            return stock_info
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
            return stock_info
    
    def validate_and_load_data(self, stock_code: str, data_dir: str) -> Optional[pd.DataFrame]:
        """éªŒè¯å¹¶åŠ è½½è‚¡ç¥¨æ•°æ®"""
        try:
            normalized_code = str(stock_code).zfill(6)
            data_path = Path(data_dir)
            csv_file = data_path / f"{normalized_code}.csv"
            
            if not csv_file.exists():
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
                return None
            
            file_size = csv_file.stat().st_size
            if file_size < 1000:
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶è¿‡å°: {csv_file} ({file_size} bytes)")
                return None
            
            df = pd.read_csv(csv_file)
            
            required_columns = ['date', 'open', 'high', 'low', 'close']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦åˆ— {missing_columns}: {stock_code}")
                return None
            
            if len(df) < 100:
                logger.warning(f"âš ï¸ æ•°æ®è¡Œæ•°ä¸è¶³: {stock_code} ({len(df)} rows)")
                return None
            
            df = df.dropna(subset=required_columns)
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
            
            price_columns = ['open', 'high', 'low', 'close']
            for col in price_columns:
                if (df[col] <= 0).any():
                    logger.warning(f"âš ï¸ å‘ç°éæ­£ä»·æ ¼æ•°æ®: {stock_code}")
                    df = df[df[col] > 0]
            
            invalid_rows = (df['high'] < df['low']) | (df['high'] < df['open']) | (df['high'] < df['close']) | \
                          (df['low'] > df['open']) | (df['low'] > df['close'])
            if invalid_rows.any():
                logger.warning(f"âš ï¸ å‘ç°ä»·æ ¼é€»è¾‘é”™è¯¯: {stock_code}")
                df = df[~invalid_rows]
            
            if len(df) < 50:
                logger.warning(f"âš ï¸ æ¸…æ´—åæ•°æ®ä¸è¶³: {stock_code} ({len(df)} rows)")
                return None
            
            logger.debug(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: {stock_code} ({len(df)} rows)")
            return df
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ {stock_code}: {e}")
            return None
    
    def zigzag(self, high_prices: np.ndarray, low_prices: np.ndarray, 
               threshold_pct: float = 0.49) -> List[Tuple[int, float, str]]:
        """
        å®ç°ZigZagæŒ‡æ ‡ç®—æ³•ï¼Œæ‰¾åˆ°æ˜¾è‘—çš„è½¬æŠ˜ç‚¹
        """
        if len(high_prices) < 3:
            return []
        
        pivots = []
        last_pivot_idx = 0
        last_pivot_price = low_prices[0]
        last_pivot_type = 'low'
        searching_for = 'high'
        
        for i in range(1, len(high_prices)):
            if searching_for == 'high':
                current_high = high_prices[i]
                if last_pivot_type == 'low':
                    pct_change = (current_high - last_pivot_price) / last_pivot_price
                    if pct_change >= threshold_pct:
                        pivots.append((last_pivot_idx, last_pivot_price, 'low'))
                        last_pivot_idx = i
                        last_pivot_price = current_high
                        last_pivot_type = 'high'
                        searching_for = 'low'
                    else:
                        if low_prices[i] < last_pivot_price:
                            last_pivot_idx = i
                            last_pivot_price = low_prices[i]
            else:
                current_low = low_prices[i]
                if last_pivot_type == 'high':
                    pct_change = (last_pivot_price - current_low) / last_pivot_price
                    if pct_change >= threshold_pct:
                        pivots.append((last_pivot_idx, last_pivot_price, 'high'))
                        last_pivot_idx = i
                        last_pivot_price = current_low
                        last_pivot_type = 'low'
                        searching_for = 'high'
                    else:
                        if high_prices[i] > last_pivot_price:
                            last_pivot_idx = i
                            last_pivot_price = high_prices[i]
        
        if pivots and last_pivot_idx != pivots[-1][0]:
            pivots.append((last_pivot_idx, last_pivot_price, last_pivot_type))
        
        return pivots

    def find_stage_lows_unified(self, df: pd.DataFrame) -> List[Tuple[int, float, str]]:
        """
        ç»Ÿä¸€ç‰ˆé˜¶æ®µä½ç‚¹æ£€æµ‹ - ä½¿ç”¨ZigZag(L,49)ç®—æ³•
        """
        try:
            threshold_pct = self.zigzag_period / 100.0
            logger.debug(f"ğŸ” å¼€å§‹ZigZagé˜¶æ®µä½ç‚¹æ£€æµ‹ (é˜ˆå€¼={self.zigzag_period}%)")
            
            high_prices = df['high'].values
            low_prices = df['low'].values
            
            pivots = self.zigzag(high_prices, low_prices, threshold_pct)
            
            stage_lows = []
            
            if not pivots:
                logger.warning("âš ï¸ ZigZagæœªæ‰¾åˆ°è½¬æŠ˜ç‚¹ï¼Œä½¿ç”¨å…¨å±€æœ€ä½ç‚¹")
                min_idx = df['low'].idxmin()
                min_price = df.loc[min_idx, 'low']
                min_date = df.loc[min_idx, 'date']
                
                if hasattr(min_date, 'strftime'):
                    min_date_str = min_date.strftime("%Y-%m-%d")
                else:
                    min_date_str = str(min_date)
                
                stage_lows = [(min_idx, min_price, min_date_str)]
            else:
                low_pivots = [(idx, price, pivot_type) for idx, price, pivot_type in pivots if pivot_type == 'low']
                
                if not low_pivots:
                    logger.warning("âš ï¸ ZigZagæœªæ‰¾åˆ°ä½ç‚¹è½¬æŠ˜ï¼Œä½¿ç”¨å…¨å±€æœ€ä½ç‚¹")
                    min_idx = df['low'].idxmin()
                    min_price = df.loc[min_idx, 'low']
                    min_date = df.loc[min_idx, 'date']
                else:
                    idx, price, _ = low_pivots[-1]
                    low_date = df.loc[idx, 'date']
                    
                    logger.debug(f"âœ… ZigZagæ‰¾åˆ° {len(low_pivots)} ä¸ªä½ç‚¹è½¬æŠ˜")
                    logger.debug(f"âœ… ZigZagæœ€è¿‘ä½ç‚¹: ç´¢å¼•={idx}, ä»·æ ¼={price:.2f}")
                    
                    if idx < len(df) - 1:
                        after_low_df = df.iloc[idx+1:]
                        after_min_idx = after_low_df['low'].idxmin()
                        after_min_price = after_low_df.loc[after_min_idx, 'low']
                        
                        if after_min_price < price:
                            logger.debug(f"ğŸ”½ å‘ç°æ›´ä½ä»·æ ¼: åŸä»·æ ¼={price:.2f}, æ–°ä»·æ ¼={after_min_price:.2f}")
                            idx = after_min_idx
                            price = after_min_price
                            low_date = df.loc[after_min_idx, 'date']
                            logger.debug(f"âœ… æ›´æ–°é”šå®šä½ç‚¹: ç´¢å¼•={idx}, æ—¥æœŸ={low_date}, ä»·æ ¼={price:.2f}")
                    
                    min_idx = idx
                    min_price = price
                    min_date = low_date
                
                if hasattr(min_date, 'strftime'):
                    min_date_str = min_date.strftime("%Y-%m-%d")
                else:
                    min_date_str = str(min_date)
                
                stage_lows = [(min_idx, min_price, min_date_str)]
            
            logger.debug(f"âœ… æœ€ç»ˆé˜¶æ®µä½ç‚¹: ç´¢å¼•={stage_lows[0][0]}, æ—¥æœŸ={stage_lows[0][2]}, ä»·æ ¼={stage_lows[0][1]:.2f}")
            return stage_lows
            
        except Exception as e:
            logger.error(f"âŒ ZigZagé˜¶æ®µä½ç‚¹æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            try:
                global_min_idx = df['low'].idxmin()
                global_min_price = df.loc[global_min_idx, 'low']
                global_min_date = df.loc[global_min_idx, 'date']
                
                if hasattr(global_min_date, 'strftime'):
                    global_min_date_str = global_min_date.strftime('%Y-%m-%d')
                else:
                    global_min_date_str = str(global_min_date)
                
                return [(global_min_idx, global_min_price, global_min_date_str)]
            except Exception as backup_e:
                logger.error(f"âŒ å¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥: {backup_e}")
                return []
    
    # ==================== AnchorM Lines åŠŸèƒ½å‡½æ•° ====================
    
    def get_local_extremes_around_turns(self, highs: np.ndarray, lows: np.ndarray,
                                       opens: np.ndarray, closes: np.ndarray,
                                       turns: List[Tuple[int, float, str]], window: int) -> List[Tuple[float, int]]:
        """è·å–ZigZagæ‹ç‚¹é™„è¿‘çš„å±€éƒ¨æå€¼
        
        Args:
            highs: æœ€é«˜ä»·æ•°ç»„
            lows: æœ€ä½ä»·æ•°ç»„
            opens: å¼€ç›˜ä»·æ•°ç»„ï¼ˆä¿ç•™å‚æ•°ä½†ä¸ä½¿ç”¨ï¼‰
            closes: æ”¶ç›˜ä»·æ•°ç»„
            turns: ZigZagè½¬æŠ˜ç‚¹åˆ—è¡¨ [(index, price, type)]ï¼Œtypeä¸º'high'æˆ–'low'
            window: æœç´¢çª—å£å¤§å°
        
        Returns:
            æå€¼ç‚¹åˆ—è¡¨ [(price, index)]
            
        é‡è¦é€»è¾‘ï¼š
            - å½“è½¬æŠ˜ç‚¹ç±»å‹ä¸º'high'æ—¶ï¼Œåªå–è¯¥è½¬æŠ˜ç‚¹é™„è¿‘çª—å£å†…çš„å±€éƒ¨æœ€é«˜ä»·
            - å½“è½¬æŠ˜ç‚¹ç±»å‹ä¸º'low'æ—¶ï¼Œå–çª—å£å†…æ‰€æœ‰æ”¶ç›˜ä»·ä¸­çš„æœ€å°å€¼
              ï¼ˆåªä½¿ç”¨æ”¶ç›˜ä»·ï¼Œä¸ä½¿ç”¨å¼€ç›˜ä»·å’Œæœ€ä½ä»·ï¼Œæ”¶ç›˜ä»·ä»£è¡¨å½“å¤©ä¹°å–åŒæ–¹æœ€ç»ˆå…±è¯†ï¼‰
        """
        extremes = []
        
        for turn_idx, turn_price, turn_type in turns:
            start_idx = max(0, turn_idx - window)
            end_idx = min(len(highs), turn_idx + window + 1)
            
            if turn_type == 'high':
                # é«˜ç‚¹è½¬æŠ˜ï¼šåªå–å±€éƒ¨æœ€é«˜ä»·
                window_highs = highs[start_idx:end_idx]
                local_max_idx = start_idx + np.argmax(window_highs)
                local_max_price = highs[local_max_idx]
                extremes.append((local_max_price, local_max_idx))
            else:  # turn_type == 'low'
                # ä½ç‚¹è½¬æŠ˜ï¼šå–çª—å£å†…æ‰€æœ‰æ”¶ç›˜ä»·çš„æœ€å°å€¼
                window_closes = closes[start_idx:end_idx]
                local_min_idx = start_idx + np.argmin(window_closes)
                local_min_price = closes[local_min_idx]
                extremes.append((local_min_price, local_min_idx))
        
        # å»é‡å¹¶æ’åº
        extremes = list(set(extremes))
        extremes.sort(key=lambda x: x[0])
        
        return extremes
    
    def generate_B_series(self, A: float, M: float, max_k: int, 
                          max_price: float) -> Tuple[List[float], List[int]]:
        """ç”ŸæˆBåºåˆ—: B_k = A + (A Ã— M) Ã— k
        
        ç­–ç•¥ï¼š
        1. è‡ªåŠ¨è®¡ç®—éœ€è¦å¤šå°‘æ ¹çº¿æ‰èƒ½è¦†ç›–åˆ°æœ€é«˜ä»·
        2. åœ¨æ­¤åŸºç¡€ä¸Šå†é¢å¤–ç”Ÿæˆ3æ ¹çº¿
        3. ä¸å— max_k é™åˆ¶ï¼ˆå¿½ç•¥è¯¥å‚æ•°ï¼‰
        """
        N = A * M
        
        if N <= 0:  # Må€¼ä¸º0æˆ–è´Ÿæ•°ï¼Œæ— æ³•ç”Ÿæˆåºåˆ—
            return [], []
        
        # è®¡ç®—è¦†ç›–åˆ°æœ€é«˜ä»·éœ€è¦çš„Kå€¼
        k_to_reach_max = int((max_price - A) / N) + 1
        
        # åœ¨è¦†ç›–æœ€é«˜ä»·çš„åŸºç¡€ä¸Šå†åŠ 3æ ¹çº¿
        k_final = k_to_reach_max + 3
        
        # å®‰å…¨é™åˆ¶ï¼šé˜²æ­¢Må€¼è¿‡å°å¯¼è‡´Kå€¼è¿‡å¤§ï¼ˆä¾‹å¦‚ > 1000ï¼‰
        # ä½†è¿™ä¸ªé™åˆ¶å¾ˆå®½æ¾ï¼Œä¸€èˆ¬ä¸ä¼šè§¦å‘
        if k_final > 500:
            logger.warning(f"âš ï¸ Kå€¼è¿‡å¤§({k_final})ï¼ŒMå€¼å¯èƒ½è¿‡å°({M*100:.1f}%)ï¼Œé™åˆ¶ä¸º500")
            k_final = 500
        
        B_values = []
        K_values = []
        
        for k in range(1, k_final + 1):
            B_k = A + N * k
            B_values.append(B_k)
            K_values.append(k)
        
        return B_values, K_values
    
    def score_M(self, B_values: List[float], extremes: List[Tuple[float, int]], 
                match_tolerance_ratio: float, time_decay_min_weight: float = 0.3) -> Dict:
        """å¯¹æŸä¸ªMå€¼è¿›è¡Œè¯„åˆ†ï¼ˆå«æ—¶é—´è¡°å‡å› å­ï¼‰
        
        Args:
            B_values: Båºåˆ—ä»·æ ¼åˆ—è¡¨
            extremes: List[Tuple[price, idx]] - ä»·æ ¼å’Œç´¢å¼•ï¼ˆè·é”šå®šç‚¹çš„å¤©æ•°ï¼‰
            match_tolerance_ratio: åŒ¹é…å®¹å·®æ¯”ä¾‹
            time_decay_min_weight: æ—¶é—´è¡°å‡æœ€å°æƒé‡ (0-1)ï¼Œè¶Šå°è¡°å‡è¶Šå¼º
        
        æ—¶é—´è¡°å‡è§„åˆ™:
            - é”šå®šç‚¹ä½ç½®ï¼ˆidx=0ï¼‰: æƒé‡ = 1.0
            - æœ€è¿œç‚¹ï¼ˆidx=maxï¼‰: æƒé‡ = time_decay_min_weight
            - ä¸­é—´ç‚¹: çº¿æ€§æ’å€¼
        """
        if not B_values or not extremes:
            return {'avg_score': 0, 'matches_count': 0, 'per_k_matches': []}
        
        # åˆ›å»ºä»·æ ¼åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºæŸ¥æ‰¾æ—¶é—´ä¿¡æ¯ï¼‰
        price_to_idx = {e[0]: e[1] for e in extremes}
        extreme_prices = [e[0] for e in extremes]
        extreme_prices_sorted = sorted(extreme_prices)
        
        # è®¡ç®—æ—¶é—´è¡°å‡ç³»æ•°ï¼šæœ€è¿œçš„æå€¼ç‚¹ç´¢å¼•
        max_idx = max(e[1] for e in extremes) if extremes else 1
        
        scores = []
        per_k_matches = []
        
        for k_idx, B_k in enumerate(B_values):
            upper = None
            lower = None
            
            for e_price in extreme_prices_sorted:
                if e_price >= B_k:
                    upper = e_price
                    break
            
            for e_price in reversed(extreme_prices_sorted):
                if e_price <= B_k:
                    lower = e_price
                    break
            
            selected_extremes = []
            if upper is not None:
                selected_extremes.append(upper)
            if lower is not None and lower != upper:
                selected_extremes.append(lower)
            
            if not selected_extremes:
                distances = [(abs(e_price - B_k), e_price) for e_price in extreme_prices]
                distances.sort()
                if distances:
                    selected_extremes.append(distances[0][1])
            
            k_scores = []
            for e_price in selected_extremes:
                # åŸºç¡€åŒ¹é…å¾—åˆ†ï¼ˆä»·æ ¼ç›¸ä¼¼åº¦ï¼‰
                r = abs(e_price - B_k) / B_k
                base_score = 100 * max(0, 1 - min(r / match_tolerance_ratio, 1))
                
                # æ—¶é—´è¡°å‡å› å­ï¼šç¦»é”šå®šç‚¹è¶Šè¿‘ï¼Œæƒé‡è¶Šé«˜
                e_idx = price_to_idx.get(e_price, max_idx)
                if max_idx > 0:
                    # æ—¶é—´æƒé‡ï¼šä» 1.0 (é”šå®šç‚¹) çº¿æ€§è¡°å‡åˆ° time_decay_min_weight (æœ€è¿œç‚¹)
                    decay_range = 1.0 - time_decay_min_weight
                    time_weight = 1.0 - decay_range * (e_idx / max_idx)
                else:
                    time_weight = 1.0
                
                # æœ€ç»ˆå¾—åˆ† = åŸºç¡€å¾—åˆ† Ã— æ—¶é—´æƒé‡
                final_score = base_score * time_weight
                k_scores.append(final_score)
            
            if k_scores:
                avg_k_score = sum(k_scores) / len(k_scores)
                scores.append(avg_k_score)
                per_k_matches.append({
                    'k': k_idx + 1,
                    'B_k': B_k,
                    'matched_extremes': selected_extremes,
                    'score': avg_k_score
                })
        
        avg_score = sum(scores) / len(scores) if scores else 0
        matches_count = len([s for s in scores if s > 0])
        
        return {
            'avg_score': avg_score,
            'matches_count': matches_count,
            'per_k_matches': per_k_matches
        }
    
    def select_best_M(self, M_results: Dict[float, Dict], min_matches: int,
                     prefer_higher_M: bool = True) -> Tuple[Optional[float], Optional[Dict]]:
        """ä»æ‰€æœ‰Må€™é€‰ä¸­é€‰æ‹©æœ€ä½³M"""
        valid_M = {M: result for M, result in M_results.items() 
                   if result['matches_count'] >= min_matches}
        
        if not valid_M:
            return None, None
        
        sorted_M = sorted(valid_M.items(), 
                         key=lambda x: (x[1]['avg_score'], 
                                       x[1]['matches_count'],
                                       x[0] if prefer_higher_M else -x[0]),
                         reverse=True)
        
        best_M, best_result = sorted_M[0]
        return best_M, best_result
    
    def compute_anchor_M_lines(self, df: pd.DataFrame, anchor_low: float, 
                              anchor_date, stock_code: str = "") -> Optional[Dict]:
        """è®¡ç®—æœ€ä½³Må€¼ä¸Båºåˆ—"""
        try:
            config = self.anchor_m_config
            
            if not config.get('enabled', True):
                return None
            
            # ç¡®ä¿ anchor_date æ˜¯ pd.Timestamp
            if isinstance(anchor_date, str):
                anchor_date = pd.to_datetime(anchor_date)
            
            df_after = df[df['date'] > anchor_date].copy()
            
            if len(df_after) < 10:
                logger.info(f"âš ï¸ [{stock_code}] é”šå®šæ—¥æœŸä¹‹åæ•°æ®ä¸è¶³: {len(df_after)}å¤©ï¼Œè·³è¿‡AnchorMçº¿")
                return None
            
            zigzag_percent = config.get('zigzag_percent', 10) / 100.0
            highs_after = df_after['high'].values
            lows_after = df_after['low'].values
            opens_after = df_after['open'].values
            closes_after = df_after['close'].values
            
            turns = self.zigzag(highs_after, lows_after, zigzag_percent)
            
            if not turns:
                logger.info(f"âš ï¸ [{stock_code}] é”šå®šæ—¥æœŸåæœªæ‰¾åˆ°ZigZag(10%)è½¬æŠ˜ç‚¹ï¼Œè·³è¿‡AnchorMçº¿")
                return None
            
            pivot_window = config.get('pivot_window', 3)
            extremes = self.get_local_extremes_around_turns(
                highs_after, lows_after, opens_after, closes_after, turns, pivot_window
            )
            
            if not extremes:
                logger.info(f"âš ï¸ [{stock_code}] æœªæ‰¾åˆ°å±€éƒ¨æå€¼ï¼Œè·³è¿‡AnchorMçº¿")
                return None
            
            m_range = config.get('m_range', {'start': 13.0, 'end': 9.0, 'step': -0.1})
            M_start = m_range['start']
            M_end = m_range['end']
            M_step = abs(m_range['step'])
            
            M_values = []
            M_current = M_start
            while M_current >= M_end - 0.001:
                M_values.append(M_current)
                M_current -= M_step
            
            max_k = config.get('max_k', 20)
            max_price = df_after['high'].max()
            match_tolerance = config.get('match_tolerance_ratio', 0.006)
            time_decay_min_weight = config.get('time_decay_min_weight', 0.3)
            
            M_results = {}
            
            for M_pct in M_values:
                M = M_pct / 100.0
                B_values, K_values = self.generate_B_series(anchor_low, M, max_k, max_price)
                
                if not B_values:
                    continue
                
                score_result = self.score_M(B_values, extremes, match_tolerance, time_decay_min_weight)
                M_results[M_pct] = {
                    'B_values': B_values,
                    'K_values': K_values,
                    'avg_score': score_result['avg_score'],
                    'matches_count': score_result['matches_count'],
                    'per_k_matches': score_result['per_k_matches']
                }
            
            min_matches = config.get('min_matches', 3)
            prefer_higher_M = config.get('tiebreaker_prefer_higher_M', True)
            
            # æ™ºèƒ½è°ƒæ•´æœ€å°åŒ¹é…æ•°ï¼šå¦‚æœé”šå®šç‚¹ä¹‹åæ•°æ®è¾ƒå°‘ï¼Œé™ä½è¦æ±‚
            # ä¾‹å¦‚ï¼šé”šå®šç‚¹ååªæœ‰6ä¸ªæœˆæ•°æ®ï¼Œå¯èƒ½åªæœ‰2-3ä¸ªè½¬æŠ˜ç‚¹ï¼Œè¿™æ˜¯æ­£å¸¸çš„
            days_after_anchor = len(df_after)
            if days_after_anchor < 200:  # çº¦10ä¸ªæœˆ
                adjusted_min_matches = max(1, min(2, min_matches))
                if adjusted_min_matches < min_matches:
                    logger.info(f"ğŸ“Š [{stock_code}] é”šå®šç‚¹åæ•°æ®è¾ƒå°‘({days_after_anchor}å¤©)ï¼Œ"
                               f"æœ€å°åŒ¹é…æ•°: {min_matches} â†’ {adjusted_min_matches}")
                    min_matches = adjusted_min_matches
            
            best_M, best_result = self.select_best_M(M_results, min_matches, prefer_higher_M)
            
            if best_M is None:
                # æ˜¾ç¤ºæ‰€æœ‰Må€¼çš„åŒ¹é…æƒ…å†µ
                if M_results:
                    max_matches = max(r['matches_count'] for r in M_results.values())
                    logger.info(f"âš ï¸ [{stock_code}] æœªæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„Må€¼(è¦æ±‚>={min_matches}ä¸ªåŒ¹é…ï¼Œå®é™…æœ€å¤š{max_matches}ä¸ª)ï¼Œè·³è¿‡AnchorMçº¿")
                else:
                    logger.info(f"âš ï¸ [{stock_code}] æœªæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„Må€¼(æœ€å°åŒ¹é…æ•°={min_matches})ï¼Œè·³è¿‡AnchorMçº¿")
                return None
            
            logger.debug(f"âœ… æœ€ä½³M={best_M:.1f}%, å¹³å‡åˆ†={best_result['avg_score']:.2f}, "
                        f"åŒ¹é…æ•°={best_result['matches_count']}")
            
            return {
                'best_M': best_M,
                'B_values': best_result['B_values'],
                'K_values': best_result['K_values'],
                'avg_score': best_result['avg_score'],
                'matches_count': best_result['matches_count'],
                'per_k_matches': best_result['per_k_matches'],
                'anchor_low': anchor_low,
                'anchor_date': anchor_date,
                'extremes': extremes
            }
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—AnchorMçº¿å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None

    def create_mid_chart(self, stock_code: str, stock_name: str, df: pd.DataFrame,
                         output_file: str) -> Tuple[bool, Optional[Dict]]:
        """
        åˆ›å»ºä¸­é—´å±‚å›¾è¡¨ï¼šåŸºç¡€Kçº¿å›¾ + AnchorMçº¿
        """
        try:
            # 1. æ£€æµ‹é˜¶æ®µä½ç‚¹
            stage_lows = self.find_stage_lows_unified(df)
            if not stage_lows:
                logger.warning(f"âš ï¸ æœªæ£€æµ‹åˆ°é˜¶æ®µä½ç‚¹: {stock_code}")
                return False, None
            
            # 2. è®¡ç®—AnchorMçº¿æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            m_lines_result = None
            if self.anchor_m_config.get('enabled', True):
                anchor_idx, anchor_low, anchor_date = stage_lows[0]
                m_lines_result = self.compute_anchor_M_lines(df, anchor_low, anchor_date, stock_code)
            
            # 3. ç»˜åˆ¶å›¾è¡¨
            with matplotlib_lock:
                import mplfinance as mpf
                
                # å‡†å¤‡æ•°æ®
                if stage_lows:
                    lowest_idx, _, _ = stage_lows[0]
                    df_display = df.iloc[lowest_idx:].copy()
                else:
                    df_display = df.copy()
                
                # é™åˆ¶æ˜¾ç¤ºçš„Kçº¿æ•°é‡ï¼Œé¿å…"æ•°æ®å¤ªå¤š"è­¦å‘Š
                max_candles = 750
                if len(df_display) > max_candles:
                    logger.info(f"ğŸ“Š [{stock_code}] æ•°æ®é‡å¤§({len(df_display)}æ ¹Kçº¿)ï¼Œ"
                               f"åªæ˜¾ç¤ºæœ€è¿‘{max_candles}æ ¹")
                    df_display = df_display.iloc[-max_candles:].copy()
                
                df_mpf = df_display.copy()
                df_mpf['date'] = pd.to_datetime(df_mpf['date'])
                df_mpf.set_index('date', inplace=True)
                df_mpf = df_mpf[['open', 'high', 'low', 'close']].copy()
                
                if df_mpf.empty:
                    logger.warning(f"âš ï¸ å¤„ç†åçš„æ•°æ®ä¸ºç©º: {stock_code}")
                    return False, None
                
                logger.info(f"ğŸ“Š [{stock_code}] ç»˜åˆ¶{len(df_mpf)}æ ¹Kçº¿")
                
                # å‡†å¤‡é¢å¤–çš„ç»˜å›¾å…ƒç´ 
                additional_plots = []
                
                # æ·»åŠ é˜¶æ®µä½ç‚¹æ°´å¹³çº¿
                for i, (idx, price, date_str) in enumerate(stage_lows):
                    hline_data = [price] * len(df_mpf)
                    additional_plots.append(mpf.make_addplot(hline_data, color='blue', linestyle='-', width=2, alpha=0.8))
                
                # æ·»åŠ ç™¾åˆ†æ¯”æ¶¨å¹…çº¿
                if stage_lows:
                    base_price = min(price for _, price, _ in stage_lows)
                    max_price = df_mpf['high'].max()
                    
                    visible_percent_lines = []
                    highest_visible_idx = -1
                    
                    for i, percent_str in enumerate(self.percent_list):
                        try:
                            percent = float(percent_str.rstrip('%')) / 100
                            target_price = base_price * (1 + percent)
                            
                            if target_price <= max_price:
                                visible_percent_lines.append((percent_str, target_price))
                                highest_visible_idx = i
                                hline_data = [target_price] * len(df_mpf)
                                additional_plots.append(mpf.make_addplot(hline_data, color='hotpink', linestyle='--', width=3, alpha=0.8))
                        except (ValueError, TypeError):
                            continue
                
                    if highest_visible_idx >= 0 and highest_visible_idx + 1 < len(self.percent_list):
                        try:
                            next_percent_str = self.percent_list[highest_visible_idx + 1]
                            next_percent = float(next_percent_str.rstrip('%')) / 100
                            next_target_price = base_price * (1 + next_percent)
                            hline_data = [next_target_price] * len(df_mpf)
                            additional_plots.append(mpf.make_addplot(hline_data, color='hotpink', linestyle='--', width=3, alpha=0.8))
                            visible_percent_lines.append((next_percent_str, next_target_price))
                        except (ValueError, TypeError):
                            pass
                
                # æ„å»ºæ ‡é¢˜ï¼ˆåŒ…å«è¡Œä¸šã€æ€»å¸‚å€¼ã€å¸‚ç›ˆç‡ï¼‰
                industry = ""
                pe_val = 0
                total_share = 0
                total_market_cap = 0
                
                if stock_code in self.stock_info:
                    info = self.stock_info[stock_code]
                    industry = info.get('industry', '')
                    pe_val = float(info.get('pe', 0))
                    total_share = float(info.get('total_share', 0))
                    
                    # è®¡ç®—æ€»å¸‚å€¼ = æ€»è‚¡æœ¬ï¼ˆäº¿è‚¡ï¼‰Ã— å½“å‰è‚¡ä»·ï¼ˆå…ƒï¼‰
                    if total_share > 0 and len(df_mpf) > 0:
                        current_price = float(df_mpf['close'].iloc[-1])
                        total_market_cap = total_share * current_price  # æ€»å¸‚å€¼ï¼ˆäº¿å…ƒï¼‰
                
                title_parts = [stock_code, stock_name]
                
                # æ·»åŠ è¡Œä¸š
                if industry and industry != "æœªçŸ¥è¡Œä¸š":
                    title_parts.append(f"({industry})")
                
                # æ·»åŠ æ€»å¸‚å€¼
                if total_market_cap > 0:
                    if total_market_cap >= 1000:
                        title_parts.append(f"æ€»å¸‚å€¼:{total_market_cap:.0f}äº¿")
                    else:
                        title_parts.append(f"æ€»å¸‚å€¼:{total_market_cap:.1f}äº¿")
                
                # æ·»åŠ å¸‚ç›ˆç‡
                if pe_val > 0:
                    title_parts.append(f"PE:{pe_val:.2f}")
                elif pe_val == 0:
                    title_parts.append("PE:äºæŸ")
                
                title = " ".join(title_parts) + " - AnchorM"
                
                # è®¾ç½®æ ·å¼
                plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
                plt.rcParams['axes.unicode_minus'] = False
                
                style = mpf.make_mpf_style(
                    base_mpf_style='charles',
                    marketcolors=mpf.make_marketcolors(
                        up='red', down='green', edge='inherit', wick='inherit', volume='inherit'
                    ),
                    gridstyle='-', gridcolor='lightgray', y_on_right=True,
                    facecolor='white', edgecolor='black', figcolor='white',
                    rc={'font.size': 12, 'axes.titlesize': 20, 'axes.labelsize': 14, 
                        'font.sans-serif': ['SimHei', 'Arial Unicode MS', 'DejaVu Sans'],
                        'axes.unicode_minus': False}
                )
                
                # åˆ›å»ºå›¾è¡¨
                fig, axes = mpf.plot(
                    df_mpf, type='candle', style=style, title=title, ylabel='Price',
                    volume=False, addplot=additional_plots if additional_plots else None,
                    figsize=(20, 12), tight_layout=True, returnfig=True,
                    panel_ratios=(1,), show_nontrading=False, 
                    datetime_format='%Y-%m', xrotation=45
                )
                
                ax = axes[0]
                
                # è°ƒæ•´Yè½´èŒƒå›´ï¼ˆéœ€è¦åœ¨ç»˜åˆ¶AnchorMçº¿ä¹‹åè°ƒæ•´ï¼‰
                # å…ˆæš‚æ—¶ä¸è®¾ç½®ï¼Œç­‰ç»˜åˆ¶å®ŒAnchorMçº¿åå†ç»Ÿä¸€è®¾ç½®
                
                # æ ‡æ³¨é˜¶æ®µä½ç‚¹ä»·æ ¼
                for i, (idx, price, date_str) in enumerate(stage_lows):
                    ax.text(1.02, price, f'{price:.2f}', 
                           fontsize=16, color='blue', fontweight='bold',
                           bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8),
                           transform=ax.get_yaxis_transform(), ha='left', va='center')
                
                # æ ‡æ³¨ç™¾åˆ†æ¯”æ¶¨å¹…çº¿
                if stage_lows:
                    base_price = min(price for _, price, _ in stage_lows)
                    max_price = df_mpf['high'].max()
                    
                    highest_visible_idx = -1
                    for i, percent_str in enumerate(self.percent_list):
                        try:
                            percent = float(percent_str.rstrip('%')) / 100
                            target_price = base_price * (1 + percent)
                            
                            if target_price <= max_price:
                                highest_visible_idx = i
                                ax.text(1.02, target_price, f'+{percent_str}', 
                                       fontsize=18, color='#8B7355', fontweight='bold',
                                       bbox=dict(boxstyle="round,pad=0.3", facecolor='white', 
                                                alpha=0.9, edgecolor='#8B7355', linewidth=2),
                                       transform=ax.get_yaxis_transform(), ha='left', va='center')
                        except (ValueError, TypeError):
                            continue
                    
                    if highest_visible_idx >= 0 and highest_visible_idx + 1 < len(self.percent_list):
                        try:
                            next_percent_str = self.percent_list[highest_visible_idx + 1]
                            next_percent = float(next_percent_str.rstrip('%')) / 100
                            next_target_price = base_price * (1 + next_percent)
                            
                            ax.text(1.02, next_target_price, f'+{next_percent_str}', 
                                   fontsize=18, color='#8B7355', fontweight='bold',
                                   bbox=dict(boxstyle="round,pad=0.3", facecolor='white', 
                                            alpha=0.9, edgecolor='#8B7355', linewidth=2),
                                   transform=ax.get_yaxis_transform(), ha='left', va='center')
                        except (ValueError, TypeError):
                            pass
                
                # 4. æ·»åŠ AnchorMçº¿
                if m_lines_result:
                    best_M = m_lines_result['best_M']
                    B_values = m_lines_result['B_values']
                    K_values = m_lines_result['K_values']
                    
                    line_style = self.anchor_m_config.get('line_style', {})
                    line_color = line_style.get('color', '#8A2BE2')
                    line_width = line_style.get('linewidth', 3.0)
                    line_alpha = line_style.get('alpha', 0.9)
                    
                    text_style = self.anchor_m_config.get('text_style', {})
                    text_fontsize = text_style.get('fontsize', 14)
                    annotate_format = self.anchor_m_config.get('annotate_format', 'K={K} ä»·æ ¼={price}')
                    
                    # ç»˜åˆ¶ç´«è‰²æ¨ªçº¿
                    for k_val, B_k_price in zip(K_values, B_values):
                        ax.axhline(y=B_k_price, color=line_color, 
                                  linestyle='-', linewidth=line_width, 
                                  alpha=line_alpha, zorder=2.5)
                        
                        label_text = annotate_format.replace('{K}', str(k_val)).replace('{price}', f'{B_k_price:.2f}')
                        ax.text(-0.02, B_k_price, label_text,
                               fontsize=text_fontsize, color=line_color, fontweight='bold',
                               bbox=dict(boxstyle="round,pad=0.4", facecolor='white', alpha=0.85, 
                                        edgecolor=line_color, linewidth=2),
                               transform=ax.get_yaxis_transform(), ha='right', va='center')
                    
                    # åœ¨å›¾ç‰‡å·¦ä¸Šè§’æ·»åŠ Må€¼ä¿¡æ¯ - åªæ˜¾ç¤ºåŒ¹é…çš„Bå€¼
                    text_lines = [f"M={best_M:.1f}%"]
                    
                    # æå–å¾—åˆ† > 0 çš„ B å€¼ï¼ˆä¸æå€¼ç‚¹åŒ¹é…çš„ï¼‰
                    if 'per_k_matches' in m_lines_result:
                        matched_B = []
                        for match in m_lines_result['per_k_matches']:
                            if match.get('score', 0) > 0:
                                k_val = match['k']
                                B_k = match['B_k']
                                score = match['score']
                                matched_B.append(f"k{k_val}:{B_k:.2f}({score:.0f})")
                                if len(matched_B) >= 10:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                                    break
                        
                        if matched_B:
                            if len(m_lines_result['per_k_matches']) > len(matched_B):
                                matched_B.append('...')
                            text_lines.append(f"Match_B: [{', '.join(matched_B)}]")
                        else:
                            text_lines.append(f"Match_B: [æ— åŒ¹é…]")
                    
                    text_lines.append(f"AvgScore: {m_lines_result['avg_score']:.1f}")
                    text_lines.append(f"Matches: {m_lines_result['matches_count']}/{len(B_values)}")
                    
                    text_content = '\n'.join(text_lines)
                    ax.text(0.01, 0.98, text_content,
                           transform=ax.transAxes,
                           fontsize=11, color='purple', fontweight='bold',
                           bbox=dict(boxstyle="round,pad=0.5", facecolor='white', alpha=0.95, 
                                    edgecolor='purple', linewidth=2.5),
                           ha='left', va='top', family='monospace')
                    
                    logger.info(f"âœ… [{stock_code}] ç»˜åˆ¶AnchorMçº¿: M={best_M:.1f}%, {len(B_values)}æ¡çº¿")
                
                # 4.5 ç»Ÿä¸€è°ƒæ•´Yè½´èŒƒå›´ï¼ˆè€ƒè™‘ç™¾åˆ†æ¯”çº¿å’ŒAnchorMçº¿ï¼‰
                if stage_lows:
                    base_price = min(price for _, price, _ in stage_lows)
                    max_price = df_mpf['high'].max()
                    min_price = df_mpf['low'].min()
                    
                    # è®¡ç®—æœ€é«˜çš„ç™¾åˆ†æ¯”çº¿
                    highest_percent_price = max_price
                    highest_visible_idx = -1
                    
                    for i, percent_str in enumerate(self.percent_list):
                        try:
                            percent = float(percent_str.rstrip('%')) / 100
                            target_price = base_price * (1 + percent)
                            if target_price <= max_price:
                                highest_visible_idx = i
                                highest_percent_price = target_price
                        except (ValueError, TypeError):
                            continue
                    
                    # åŠ ä¸ŠKçº¿ä¸Šæ–¹çš„é¢å¤–ç™¾åˆ†æ¯”çº¿
                    if highest_visible_idx >= 0 and highest_visible_idx + 1 < len(self.percent_list):
                        try:
                            next_percent_str = self.percent_list[highest_visible_idx + 1]
                            next_percent = float(next_percent_str.rstrip('%')) / 100
                            next_target_price = base_price * (1 + next_percent)
                            highest_percent_price = next_target_price
                        except (ValueError, TypeError):
                            pass
                    
                    # è€ƒè™‘AnchorMçº¿çš„æœ€é«˜ä»·æ ¼
                    highest_line_price = highest_percent_price
                    if m_lines_result and m_lines_result['B_values']:
                        highest_m_price = max(m_lines_result['B_values'])
                        highest_line_price = max(highest_percent_price, highest_m_price)
                    
                    # è®¾ç½®Yè½´èŒƒå›´ï¼Œç¡®ä¿æ‰€æœ‰çº¿éƒ½å¯è§
                    y_margin = (highest_line_price - min_price) * 0.05
                    ax.set_ylim(min_price - y_margin, highest_line_price + y_margin)
                    logger.debug(f"ğŸ“Š [{stock_code}] Yè½´èŒƒå›´: {min_price:.2f} - {highest_line_price:.2f}")
                
                # 4.6 ç»˜åˆ¶æœ€åä¸€ä¸ªäº¤æ˜“æ—¥çš„æ”¶ç›˜ä»·æ¨ªçº¿
                last_close_price = df_mpf['close'].iloc[-1]
                ax.axhline(y=last_close_price, color='red', linestyle='-', linewidth=3, alpha=0.8, zorder=3)
                
                # åœ¨å³ä¾§æ ‡æ³¨æ”¶ç›˜ä»·
                ax.text(1.02, last_close_price, f'{last_close_price:.2f}', 
                       fontsize=16, color='red', fontweight='bold',
                       transform=ax.get_yaxis_transform(), ha='left', va='center')
                logger.debug(f"ğŸ“Š [{stock_code}] æœ€åäº¤æ˜“æ—¥æ”¶ç›˜ä»·æ¨ªçº¿: {last_close_price:.2f}")
                
                # 5. ä¿å­˜å›¾è¡¨
                plt.savefig(output_file, dpi=200, bbox_inches='tight', facecolor='white', edgecolor='none')
                plt.close(fig)
                
                # 6. è°ƒæ•´å›¾ç‰‡å°ºå¯¸
                try:
                    from PIL import Image
                    with Image.open(output_file) as img:
                        target_width = 3991
                        target_height = 2392
                        resized_img = img.resize((target_width, target_height), Image.Resampling.LANCZOS)
                        resized_img.save(output_file, 'PNG', quality=95)
                        logger.debug(f"âœ… å›¾ç‰‡å°ºå¯¸å·²è°ƒæ•´: {target_width}x{target_height}")
                except ImportError:
                    logger.warning("âš ï¸ PILæœªå®‰è£…ï¼Œæ— æ³•è°ƒæ•´å›¾ç‰‡å°ºå¯¸")
                except Exception as e:
                    logger.warning(f"âš ï¸ è°ƒæ•´å›¾ç‰‡å°ºå¯¸å¤±è´¥: {e}")
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸç”Ÿæˆ
                if os.path.exists(output_file):
                    file_size = os.path.getsize(output_file)
                    if file_size > 1000:
                        logger.debug(f"âœ… å›¾è¡¨ç”ŸæˆæˆåŠŸ: {output_file} ({file_size} bytes)")
                        return True, m_lines_result
                    else:
                        logger.warning(f"âš ï¸ ç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶è¿‡å°: {output_file} ({file_size} bytes)")
                        return False, None
                else:
                    logger.error(f"âŒ å›¾è¡¨æ–‡ä»¶æœªç”Ÿæˆ: {output_file}")
                    return False, None
                    
        except Exception as e:
            logger.error(f"âŒ å›¾è¡¨åˆ›å»ºå¤±è´¥ {stock_code}: {type(e).__name__}: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False, None
        finally:
            try:
                plt.close('all')
            except:
                pass
    
    def process_stock_list(self, stock_list: List[Tuple[str, str, str, str]], 
                          output_dir: Optional[str] = None, data_dir: str = "../data", workers: int = 4):
        """å¤„ç†æŒ‡å®šçš„è‚¡ç¥¨åˆ—è¡¨"""
        if output_dir is None:
            current_date = datetime.now().strftime('%Y%m%d')
            output_dir = f'{current_date}-drawLineMid'
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†è‚¡ç¥¨åˆ—è¡¨ï¼ˆä¸­é—´å±‚ï¼‰")
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"ğŸ§µ çº¿ç¨‹æ•°: {workers}")
        
        if not stock_list:
            logger.error("âŒ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
            return
        
        self.total_count = len(stock_list)
        self.processed_count = 0
        
        logger.info(f"ğŸ“Š å¾…å¤„ç†è‚¡ç¥¨æ•°é‡: {self.total_count}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆè¦†ç›–æ¨¡å¼ï¼Œä¸æ¸…ç©ºï¼‰
        os.makedirs(output_dir, exist_ok=True)
        if os.path.exists(output_dir) and os.listdir(output_dir):
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•å·²å­˜åœ¨: {output_dir}ï¼ˆå°†è¦†ç›–åŒåæ–‡ä»¶ï¼‰")
        else:
            logger.info(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        
        # å¤šçº¿ç¨‹å¤„ç†
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=workers) as executor:
            future_to_stock = {
                executor.submit(self._process_single_stock, code, name, output_dir, data_dir, file_prefix): (code, name, industry, file_prefix)
                for code, name, industry, file_prefix in stock_list
            }
            
            for future in as_completed(future_to_stock):
                result = future.result()
                results.append(result)
        
        # ç»Ÿè®¡ç»“æœ
        total_time = time.time() - start_time
        success_count = sum(1 for r in results if r['success'])
        failed_count = len(results) - success_count
        
        logger.info(f"ğŸ‰ è‚¡ç¥¨åˆ—è¡¨å¤„ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»è®¡: {len(results)}åªè‚¡ç¥¨")
        logger.info(f"âœ… æˆåŠŸ: {success_count}åª")
        logger.info(f"âŒ å¤±è´¥: {failed_count}åª")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {len(results)/total_time:.2f}åª/ç§’")
        
        # ä¿å­˜å¤„ç†ç»“æœ
        results_file = os.path.join(output_dir, "processing_results.json")
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“„ å¤„ç†ç»“æœå·²ä¿å­˜: {results_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„è‚¡ç¥¨
        failed_stocks = [r for r in results if not r['success']]
        if failed_stocks:
            logger.warning(f"âš ï¸ å¤±è´¥çš„è‚¡ç¥¨:")
            for r in failed_stocks[:10]:
                logger.warning(f"   {r['stock_code']} {r['stock_name']}: {r['error']}")
            if len(failed_stocks) > 10:
                logger.warning(f"   ... è¿˜æœ‰{len(failed_stocks)-10}åªè‚¡ç¥¨å¤±è´¥")

    def _process_single_stock(self, stock_code: str, stock_name: str, 
                           output_dir: str, data_dir: str, file_prefix: str = "") -> dict:
        """å¤„ç†å•åªè‚¡ç¥¨ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        start_time = time.time()
        result = {
            'stock_code': stock_code,
            'stock_name': stock_name,
            'success': False,
            'elapsed_time': 0,
            'error': None
        }
        
        try:
            # 1. åŠ è½½æ•°æ®
            df = self.validate_and_load_data(stock_code, data_dir)
            if df is None:
                result['error'] = "æ•°æ®åŠ è½½å¤±è´¥"
                return result
            
            # 2. åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # 3. ç”Ÿæˆå›¾è¡¨
            if file_prefix and file_prefix != "UNKNOWN":
                # æ ¼å¼: {å‰ç¼€}_{è¡Œä¸š}_{è‚¡ç¥¨åç§°}_{è‚¡ç¥¨ä»£ç }_1mid.png
                output_file = os.path.join(output_dir, f"{file_prefix}_{industry}_{stock_name}_{stock_code}_1mid.png")
            else:
                # æ— å‰ç¼€æ—¶: {è‚¡ç¥¨ä»£ç }_{è‚¡ç¥¨åç§°}_1mid.png
                output_file = os.path.join(output_dir, f"{stock_code}_{stock_name}_1mid.png")
            
            success, m_lines_result = self.create_mid_chart(stock_code, stock_name, df, output_file)
            
            if success:
                result['success'] = True
                
                # æ·»åŠ AnchorMçº¿ç»“æœ
                if m_lines_result:
                    result['anchorMLines'] = {
                        'best_M': m_lines_result['best_M'],
                        'avg_score': m_lines_result['avg_score'],
                        'matches_count': m_lines_result['matches_count'],
                        'B_values': m_lines_result['B_values'][:10],
                        'anchor_low': m_lines_result['anchor_low'],
                        'anchor_date': str(m_lines_result['anchor_date'])
                    }
                
                # æ›´æ–°è¿›åº¦
                with progress_lock:
                    self.processed_count += 1
                    m_info = f", M={m_lines_result['best_M']:.1f}%" if m_lines_result else ""
                    logger.info(f"âœ… [{self.processed_count}/{self.total_count}] {stock_code} {stock_name}{m_info}")
            else:
                result['error'] = "å›¾è¡¨åˆ›å»ºå¤±è´¥"
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"âŒ å¤„ç†è‚¡ç¥¨å¤±è´¥ {stock_code}: {e}")
        finally:
            result['elapsed_time'] = time.time() - start_time
        
        return result


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸­é—´å±‚ç”»çº¿è„šæœ¬ - åŸºç¡€å›¾è¡¨ + AnchorMçº¿",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†å½“å‰æ—¥æœŸçš„resByFilterä¸­çš„è‚¡ç¥¨
  python draw_lines_mid.py
  
  # å¤„ç†æŒ‡å®šæ—¥æœŸçš„resByFilterä¸­çš„è‚¡ç¥¨
  python draw_lines_mid.py --date 2025-10-20
  
  # æŒ‡å®šçº¿ç¨‹æ•°
  python draw_lines_mid.py --date 2025-10-20 --workers 6
  
  # å¤„ç†æŒ‡å®šè‚¡ç¥¨ä»£ç 
  python draw_lines_mid.py --codes 000001 600000 002603
        """
    )
    
    current_date = datetime.now().strftime('%Y%m%d')
    
    parser.add_argument('--date', type=str, 
                       help='æ—¥æœŸå‚æ•°ï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œç”¨äºæ„å»ºresByFilterç›®å½•')
    parser.add_argument('--workers', type=int, default=4,
                       help='å¹¶å‘å¤„ç†çš„çº¿ç¨‹æ•° (é»˜è®¤: 4)')
    parser.add_argument('--codes', nargs='+', type=str,
                       help='è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¤šä¸ªä»£ç ç”¨ç©ºæ ¼åˆ†éš”ï¼ˆå¦‚ï¼š000001 600000ï¼‰')
    
    args = parser.parse_args()
    
    # å¤„ç†æ—¥æœŸå‚æ•°
    if args.date:
        try:
            date_obj = datetime.strptime(args.date, '%Y-%m-%d')
            date_str = date_obj.strftime('%Y%m%d')
        except ValueError:
            logger.error(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {args.date}ï¼Œè¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼")
            sys.exit(1)
    else:
        date_str = current_date
    
    # åˆ›å»ºä¸­é—´å±‚ç”»çº¿å™¨
    drawer = MidLineDrawer()
    
    # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œç›´æ¥ä»dataç›®å½•è¯»å–
    if args.codes:
        logger.info(f"ğŸ“Š å¤„ç†æŒ‡å®šçš„è‚¡ç¥¨ä»£ç : {', '.join(args.codes)}")
        stock_list = []
        
        # è¯»å–stocklist.csvè·å–è‚¡ç¥¨åç§°å’Œè¡Œä¸šä¿¡æ¯
        stocklist_path = "../stocklist.csv"
        stock_info_dict = {}
        if os.path.exists(stocklist_path):
            try:
                stocklist_df = pd.read_csv(stocklist_path)
                for _, row in stocklist_df.iterrows():
                    code = str(row.get('symbol', '')).zfill(6)
                    name = str(row.get('name', code))
                    industry = str(row.get('industry', 'æœªçŸ¥è¡Œä¸š'))
                    stock_info_dict[code] = (name, industry)
                logger.info(f"âœ… å·²åŠ è½½ {len(stock_info_dict)} åªè‚¡ç¥¨çš„åŸºç¡€ä¿¡æ¯")
            except Exception as e:
                logger.warning(f"âš ï¸ è¯»å–stocklist.csvå¤±è´¥: {e}")
        
        # å¤„ç†æ¯ä¸ªè‚¡ç¥¨ä»£ç 
        for code in args.codes:
            normalized_code = code.zfill(6)
            
            # è·å–è‚¡ç¥¨åç§°å’Œè¡Œä¸š
            if normalized_code in stock_info_dict:
                name, industry = stock_info_dict[normalized_code]
            else:
                name = normalized_code
                industry = "æœªçŸ¥è¡Œä¸š"
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨ {normalized_code} çš„åŸºç¡€ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            
            stock_list.append((normalized_code, name, industry, ""))
        
        logger.info(f"ğŸ“‹ å…±æœ‰ {len(stock_list)} åªè‚¡ç¥¨å¾…å¤„ç†")
        
        # ç”Ÿæˆè¾“å‡ºç›®å½•
        output_dir = f"{date_str}-drawLineMid"
        
        # æ‰¹é‡å¤„ç†è‚¡ç¥¨åˆ—è¡¨
        drawer.process_stock_list(stock_list, output_dir, "../data", args.workers)
        
    else:
        # åŸæœ‰é€»è¾‘ï¼šä»resByFilterè¯»å–è‚¡ç¥¨
        filter_dir = f"../{date_str}-resByFilter"
        if not os.path.exists(filter_dir):
            logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {filter_dir}")
            logger.info(f"ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿å­˜åœ¨ {filter_dir} ç›®å½•ï¼Œæˆ–ä½¿ç”¨ --codes å‚æ•°æŒ‡å®šè‚¡ç¥¨ä»£ç ")
            sys.exit(1)
        
        # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = glob.glob(os.path.join(filter_dir, "*.csv"))
        if not csv_files:
            logger.error(f"âŒ åœ¨ç›®å½• {filter_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            sys.exit(1)
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        # å®šä¹‰å‰ç¼€ä¼˜å…ˆçº§å‡½æ•°(æ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜)
        def get_prefix_priority(prefix):
            """æå–å‰ç¼€ä¸­çš„æ•°å­—,æ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜"""
            import re
            match = re.search(r'(\d+)', prefix)
            if match:
                return int(match.group(1))
            return 999  # æ²¡æœ‰æ•°å­—çš„å‰ç¼€ä¼˜å…ˆçº§æœ€ä½
        
        # è¯»å–æ‰€æœ‰CSVæ–‡ä»¶ä¸­çš„è‚¡ç¥¨ï¼Œå¹¶å»é‡(ä¿ç•™ä¼˜å…ˆçº§æœ€é«˜çš„å‰ç¼€)
        all_stocks = {}  # ä½¿ç”¨å­—å…¸å»é‡ï¼Œkeyä¸ºè‚¡ç¥¨ä»£ç , valueä¸º(code, name, industry, prefix)
        
        for file_path in csv_files:
            logger.info(f"ğŸ“„ è¯»å–æ–‡ä»¶: {file_path}")
            try:
                df = pd.read_csv(file_path)
                
                # ä»æ–‡ä»¶åæå–å‰ç¼€
                file_name = os.path.basename(file_path)
                file_prefix = ""
                
                import re
                patterns = [
                    (r'^ADX(\d+)', 'ADX'),
                    (r'^PDI(\d+)', 'PDI'),
                    (r'ADX(\d+)', 'ADX'),
                    (r'PDI(\d+)', 'PDI')
                ]
                
                for pattern, prefix_type in patterns:
                    match = re.search(pattern, file_name.upper())
                    if match:
                        file_prefix = f"{prefix_type}{match.group(1)}"
                        break
                
                logger.info(f"ğŸ“Š æ–‡ä»¶ç±»å‹: {file_prefix}")
                
                # æå–è‚¡ç¥¨ä¿¡æ¯
                for _, row in df.iterrows():
                    code = str(row.get('code', ''))
                    name = str(row.get('name', code))
                    industry = str(row.get('industry', 'æœªçŸ¥è¡Œä¸š'))
                    
                    if code:
                        normalized_code = code.zfill(6)
                        # å¦‚æœè‚¡ç¥¨å·²å­˜åœ¨,æ¯”è¾ƒä¼˜å…ˆçº§,ä¿ç•™æ•°å­—å°çš„å‰ç¼€
                        if normalized_code in all_stocks:
                            existing_prefix = all_stocks[normalized_code][3]
                            current_priority = get_prefix_priority(file_prefix)
                            existing_priority = get_prefix_priority(existing_prefix)
                            
                            if current_priority < existing_priority:
                                # å½“å‰å‰ç¼€ä¼˜å…ˆçº§æ›´é«˜,æ›¿æ¢
                                all_stocks[normalized_code] = (normalized_code, name, industry, file_prefix)
                                logger.info(f"  ğŸ“Œ [{normalized_code}] {name}: ä½¿ç”¨{file_prefix}æ›¿æ¢{existing_prefix}(ä¼˜å…ˆçº§æ›´é«˜)")
                        else:
                            # è‚¡ç¥¨ä¸å­˜åœ¨,ç›´æ¥æ·»åŠ 
                            all_stocks[normalized_code] = (normalized_code, name, industry, file_prefix)
                            
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                continue
        
        if not all_stocks:
            logger.error(f"âŒ æœªè¯»å–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®")
            sys.exit(1)
        
        stock_list = list(all_stocks.values())
        logger.info(f"ğŸ“‹ å»é‡åå…±æœ‰ {len(stock_list)} åªè‚¡ç¥¨")
        
        # ç”Ÿæˆè¾“å‡ºç›®å½•
        output_dir = f"{date_str}-drawLineMid"
        
        # æ‰¹é‡å¤„ç†è‚¡ç¥¨åˆ—è¡¨
        drawer.process_stock_list(stock_list, output_dir, "../data", args.workers)
    
    logger.info("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")


if __name__ == "__main__":
    main()
