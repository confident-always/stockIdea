#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”»çº¿æµæ°´çº¿è„šæœ¬ - åŒæ—¶è¿è¡Œ draw_lines_mid.py å’Œ draw_lines_back.py
å°†ä¸¤ç§ç®—æ³•çš„ç»“æœè¾“å‡ºåˆ°åŒä¸€ä¸ªç›®å½•ï¼Œé€šè¿‡æ–‡ä»¶ååç¼€åŒºåˆ†

åŠŸèƒ½ç‰¹æ€§ï¼š
1. åŒæ—¶è¿è¡Œ AnchorM å’Œ AnchorBack ä¸¤ç§ç®—æ³•
2. ç»Ÿä¸€è¾“å‡ºç›®å½•ï¼š{æ—¥æœŸ}-drawLine
3. æ–‡ä»¶å‘½åè§„åˆ™ï¼š
   - AnchorM: {å‰ç¼€}_{ä»£ç }_{è‚¡ç¥¨å}_1mid.png
   - AnchorBack: {å‰ç¼€}_{ä»£ç }_{è‚¡ç¥¨å}_2back.png
4. å¹¶è¡Œå¤„ç†ï¼Œæé«˜æ•ˆç‡
5. ç»Ÿä¸€çš„æ—¥å¿—å’Œç»“æœæ±‡æ€»

ä½¿ç”¨æ–¹æ³•ï¼š
    # å¤„ç†æŒ‡å®šæ—¥æœŸçš„è‚¡ç¥¨
    python run_draw_lines_pipeline.py --date 2025-10-22
    
    # æŒ‡å®šçº¿ç¨‹æ•°
    python run_draw_lines_pipeline.py --date 2025-10-22 --workers 4
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging
from datetime import datetime
import glob
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time

# å¯¼å…¥ä¸¤ä¸ªç”»çº¿å™¨ï¼ˆä½¿ç”¨åˆ«ååŒºåˆ†ï¼‰
from draw_lines_mid import MidLineDrawer as AnchorMDrawer
from draw_lines_back import MidLineDrawer as AnchorBackDrawer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] [%(threadName)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('run_draw_lines_pipeline.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# çº¿ç¨‹é”
progress_lock = threading.Lock()


class DrawLinesPipeline:
    """ç”»çº¿æµæ°´çº¿ - åŒæ—¶è¿è¡Œä¸¤ç§ç®—æ³•"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        self.mid_drawer = AnchorMDrawer()
        self.back_drawer = AnchorBackDrawer()
        self.processed_count = 0
        self.total_count = 0
        logger.info(f"âœ… ç”»çº¿æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def process_single_stock(self, stock_code: str, stock_name: str, 
                           output_dir: str, data_dir: str, file_prefix: str = "") -> dict:
        """å¤„ç†å•åªè‚¡ç¥¨ - åŒæ—¶ç”Ÿæˆä¸¤ç§ç®—æ³•çš„å›¾è¡¨"""
        start_time = time.time()
        result = {
            'stock_code': stock_code,
            'stock_name': stock_name,
            'mid_success': False,
            'back_success': False,
            'elapsed_time': 0,
            'mid_error': None,
            'back_error': None
        }
        
        try:
            # 1. åŠ è½½æ•°æ®ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼Œä¸¤ç§ç®—æ³•å…±ç”¨ï¼‰
            df = self.mid_drawer.validate_and_load_data(stock_code, data_dir)
            if df is None:
                result['mid_error'] = "æ•°æ®åŠ è½½å¤±è´¥"
                result['back_error'] = "æ•°æ®åŠ è½½å¤±è´¥"
                return result
            
            # 2. åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # 3. ç”Ÿæˆ AnchorM å›¾è¡¨ï¼ˆ1midï¼‰
            if file_prefix and file_prefix != "UNKNOWN":
                mid_output_file = os.path.join(output_dir, f"{file_prefix}_{stock_code}_{stock_name}_1mid.png")
            else:
                mid_output_file = os.path.join(output_dir, f"{stock_code}_{stock_name}_1mid.png")
            
            try:
                mid_success, mid_lines_result = self.mid_drawer.create_mid_chart(
                    stock_code, stock_name, df.copy(), mid_output_file
                )
                result['mid_success'] = mid_success
                
                if mid_success and mid_lines_result:
                    result['anchorMLines'] = {
                        'best_M': mid_lines_result['best_M'],
                        'avg_score': mid_lines_result['avg_score'],
                        'matches_count': mid_lines_result['matches_count']
                    }
            except Exception as e:
                result['mid_error'] = str(e)
                logger.error(f"âŒ AnchorMå›¾è¡¨åˆ›å»ºå¤±è´¥ {stock_code}: {e}")
            
            # 4. ç”Ÿæˆ AnchorBack å›¾è¡¨ï¼ˆ2backï¼‰
            if file_prefix and file_prefix != "UNKNOWN":
                back_output_file = os.path.join(output_dir, f"{file_prefix}_{stock_code}_{stock_name}_2back.png")
            else:
                back_output_file = os.path.join(output_dir, f"{stock_code}_{stock_name}_2back.png")
            
            try:
                back_success, back_lines_result = self.back_drawer.create_mid_chart(
                    stock_code, stock_name, df.copy(), back_output_file
                )
                result['back_success'] = back_success
                
                if back_success and back_lines_result:
                    # draw_lines_back.py è¿”å›çš„ä¹Ÿæ˜¯ M å€¼ï¼ˆå› ä¸ºå®ƒè¿˜ä½¿ç”¨ AnchorM çš„ç»“æ„ï¼‰
                    result['anchorBackLines'] = {
                        'best_M': back_lines_result.get('best_M', 0),
                        'avg_score': back_lines_result.get('avg_score', 0),
                        'matches_count': back_lines_result.get('matches_count', 0)
                    }
            except Exception as e:
                result['back_error'] = str(e)
                logger.error(f"âŒ AnchorBackå›¾è¡¨åˆ›å»ºå¤±è´¥ {stock_code}: {e}")
            
            # 5. æ›´æ–°è¿›åº¦
            with progress_lock:
                self.processed_count += 1
                status_parts = []
                if result['mid_success']:
                    m_val = result.get('anchorMLines', {}).get('best_M', 0)
                    status_parts.append(f"Mid:M={m_val:.1f}%")
                else:
                    status_parts.append("Mid:å¤±è´¥")
                
                if result['back_success']:
                    m_val_back = result.get('anchorBackLines', {}).get('best_M', 0)
                    status_parts.append(f"Back:M={m_val_back:.1f}%")
                else:
                    status_parts.append("Back:å¤±è´¥")
                
                status = " | ".join(status_parts)
                logger.info(f"âœ… [{self.processed_count}/{self.total_count}] {stock_code} {stock_name} - {status}")
            
        except Exception as e:
            result['mid_error'] = str(e)
            result['back_error'] = str(e)
            logger.error(f"âŒ å¤„ç†è‚¡ç¥¨å¤±è´¥ {stock_code}: {e}")
        finally:
            result['elapsed_time'] = time.time() - start_time
        
        return result
    
    def process_stock_list(self, stock_list: List[Tuple[str, str, str, str]], 
                          output_dir: str, data_dir: str = "../data", workers: int = 4):
        """å¤„ç†è‚¡ç¥¨åˆ—è¡¨"""
        logger.info(f"ğŸš€ å¼€å§‹ç”»çº¿æµæ°´çº¿å¤„ç†")
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"ğŸ§µ çº¿ç¨‹æ•°: {workers}")
        logger.info(f"ğŸ“Š ç®—æ³•: AnchorM (ç´«è‰²) + AnchorBack (è“è‰²)")
        
        if not stock_list:
            logger.error("âŒ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
            return
        
        self.total_count = len(stock_list)
        self.processed_count = 0
        
        logger.info(f"ğŸ“Š å¾…å¤„ç†è‚¡ç¥¨æ•°é‡: {self.total_count}")
        logger.info(f"ğŸ“Š é¢„è®¡ç”Ÿæˆå›¾ç‰‡æ•°é‡: {self.total_count * 2} å¼ ")
        
        # æ¸…ç©ºå¹¶é‡æ–°åˆ›å»ºè¾“å‡ºç›®å½•
        if os.path.exists(output_dir):
            import shutil
            logger.info(f"ğŸ—‘ï¸  æ¸…ç©ºè¾“å‡ºç›®å½•: {output_dir}")
            try:
                shutil.rmtree(output_dir)
                logger.info(f"âœ… å·²æ¸…ç©ºè¾“å‡ºç›®å½•")
            except Exception as e:
                logger.warning(f"âš ï¸  æ¸…ç©ºè¾“å‡ºç›®å½•æ—¶å‡ºé”™: {e}")
        
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        
        # å¤šçº¿ç¨‹å¤„ç†
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=workers) as executor:
            future_to_stock = {
                executor.submit(self.process_single_stock, code, name, output_dir, data_dir, file_prefix): 
                (code, name, industry, file_prefix)
                for code, name, industry, file_prefix in stock_list
            }
            
            for future in as_completed(future_to_stock):
                result = future.result()
                results.append(result)
        
        # ç»Ÿè®¡ç»“æœ
        total_time = time.time() - start_time
        mid_success_count = sum(1 for r in results if r['mid_success'])
        back_success_count = sum(1 for r in results if r['back_success'])
        both_success_count = sum(1 for r in results if r['mid_success'] and r['back_success'])
        
        logger.info(f"")
        logger.info(f"ğŸ‰ ç”»çº¿æµæ°´çº¿å¤„ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»è®¡è‚¡ç¥¨: {len(results)}åª")
        logger.info(f"âœ… AnchorMæˆåŠŸ: {mid_success_count}åª ({mid_success_count/len(results)*100:.1f}%)")
        logger.info(f"âœ… AnchorBackæˆåŠŸ: {back_success_count}åª ({back_success_count/len(results)*100:.1f}%)")
        logger.info(f"âœ… ä¸¤ç§ç®—æ³•éƒ½æˆåŠŸ: {both_success_count}åª ({both_success_count/len(results)*100:.1f}%)")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {len(results)/total_time:.2f}åª/ç§’")
        
        # ä¿å­˜å¤„ç†ç»“æœ
        results_file = os.path.join(output_dir, "pipeline_results.json")
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“„ å¤„ç†ç»“æœå·²ä¿å­˜: {results_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„è‚¡ç¥¨
        failed_stocks = [r for r in results if not (r['mid_success'] or r['back_success'])]
        if failed_stocks:
            logger.warning(f"âš ï¸ å®Œå…¨å¤±è´¥çš„è‚¡ç¥¨ï¼ˆä¸¤ç§ç®—æ³•éƒ½å¤±è´¥ï¼‰:")
            for r in failed_stocks[:10]:
                logger.warning(f"   {r['stock_code']} {r['stock_name']}")
            if len(failed_stocks) > 10:
                logger.warning(f"   ... è¿˜æœ‰{len(failed_stocks)-10}åªè‚¡ç¥¨å®Œå…¨å¤±è´¥")
        
        # æ˜¾ç¤ºéƒ¨åˆ†å¤±è´¥çš„è‚¡ç¥¨
        partial_failed = [r for r in results if (r['mid_success'] ^ r['back_success'])]
        if partial_failed:
            logger.warning(f"âš ï¸ éƒ¨åˆ†æˆåŠŸçš„è‚¡ç¥¨ï¼ˆåªæœ‰ä¸€ç§ç®—æ³•æˆåŠŸï¼‰: {len(partial_failed)}åª")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ç”»çº¿æµæ°´çº¿è„šæœ¬ - åŒæ—¶è¿è¡Œ AnchorM å’Œ AnchorBack",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†å½“å‰æ—¥æœŸçš„resByFilterä¸­çš„è‚¡ç¥¨
  python run_draw_lines_pipeline.py
  
  # å¤„ç†æŒ‡å®šæ—¥æœŸçš„resByFilterä¸­çš„è‚¡ç¥¨
  python run_draw_lines_pipeline.py --date 2025-10-22
  
  # æŒ‡å®šçº¿ç¨‹æ•°
  python run_draw_lines_pipeline.py --date 2025-10-22 --workers 6

è¾“å‡ºè¯´æ˜:
  è¾“å‡ºç›®å½•: {æ—¥æœŸ}-drawLine/
  æ–‡ä»¶å‘½å:
    - AnchorMå›¾è¡¨: {å‰ç¼€}_{ä»£ç }_{è‚¡ç¥¨å}_1mid.png
    - AnchorBackå›¾è¡¨: {å‰ç¼€}_{ä»£ç }_{è‚¡ç¥¨å}_2back.png
        """
    )
    
    current_date = datetime.now().strftime('%Y%m%d')
    
    parser.add_argument('--date', type=str, 
                       help='æ—¥æœŸå‚æ•°ï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œç”¨äºæ„å»ºresByFilterç›®å½•')
    parser.add_argument('--workers', type=int, default=4,
                       help='å¹¶å‘å¤„ç†çš„çº¿ç¨‹æ•° (é»˜è®¤: 4)')
    
    args = parser.parse_args()
    
    # å¤„ç†æ—¥æœŸå‚æ•°
    if args.date:
        try:
            date_obj = datetime.strptime(args.date, '%Y-%m-%d')
            date_str = date_obj.strftime('%Y%m%d')
        except ValueError:
            logger.error(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {args.date}ï¼Œè¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼")
            sys.exit(1)
    else:
        date_str = current_date
    
    # åˆ›å»ºæµæ°´çº¿
    pipeline = DrawLinesPipeline()
    
    # è¯»å–æŒ‡å®šæ—¥æœŸçš„resByFilterä¸­çš„è‚¡ç¥¨
    filter_dir = f"../{date_str}-resByFilter"
    if not os.path.exists(filter_dir):
        logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {filter_dir}")
        logger.info(f"ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿å­˜åœ¨ {filter_dir} ç›®å½•")
        sys.exit(1)
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = glob.glob(os.path.join(filter_dir, "*.csv"))
    if not csv_files:
        logger.error(f"âŒ åœ¨ç›®å½• {filter_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        sys.exit(1)
    
    logger.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    # è¯»å–æ‰€æœ‰CSVæ–‡ä»¶ä¸­çš„è‚¡ç¥¨ï¼Œå¹¶å»é‡
    all_stocks = {}
    
    for file_path in csv_files:
        logger.info(f"ğŸ“„ è¯»å–æ–‡ä»¶: {file_path}")
        try:
            df = pd.read_csv(file_path)
            
            # ä»æ–‡ä»¶åæå–å‰ç¼€
            file_name = os.path.basename(file_path)
            file_prefix = ""
            
            import re
            patterns = [
                (r'^ADX(\d+)', 'ADX'),
                (r'^PDI(\d+)', 'PDI'),
                (r'ADX(\d+)', 'ADX'),
                (r'PDI(\d+)', 'PDI')
            ]
            
            for pattern, prefix_type in patterns:
                match = re.search(pattern, file_name.upper())
                if match:
                    file_prefix = f"{prefix_type}{match.group(1)}"
                    break
            
            logger.info(f"ğŸ“Š æ–‡ä»¶ç±»å‹: {file_prefix}")
            
            # æå–è‚¡ç¥¨ä¿¡æ¯
            for _, row in df.iterrows():
                code = str(row.get('code', ''))
                name = str(row.get('name', code))
                industry = str(row.get('industry', 'æœªçŸ¥è¡Œä¸š'))
                
                if code:
                    normalized_code = code.zfill(6)
                    if normalized_code not in all_stocks:
                        all_stocks[normalized_code] = (normalized_code, name, industry, file_prefix)
                        
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            continue
    
    if not all_stocks:
        logger.error(f"âŒ æœªè¯»å–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®")
        sys.exit(1)
    
    stock_list = list(all_stocks.values())
    logger.info(f"ğŸ“‹ å»é‡åå…±æœ‰ {len(stock_list)} åªè‚¡ç¥¨")
    
    # ç”Ÿæˆç»Ÿä¸€è¾“å‡ºç›®å½•
    output_dir = f"{date_str}-drawLine"
    
    # æ‰¹é‡å¤„ç†è‚¡ç¥¨åˆ—è¡¨
    pipeline.process_stock_list(stock_list, output_dir, "../data", args.workers)
    
    logger.info("ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆ!")


if __name__ == "__main__":
    main()

