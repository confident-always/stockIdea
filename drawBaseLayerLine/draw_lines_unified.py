#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ç‰ˆåŸºç¡€å±‚ç”»çº¿è„šæœ¬
æ•´åˆæ‰€æœ‰drawBaseLayerLineåŠŸèƒ½ä¸ºå•ä¸€å¯æ‰§è¡Œè„šæœ¬

åŠŸèƒ½ç‰¹æ€§ï¼š
1. ä»resByFilterä¸­æå–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
2. æ™ºèƒ½æ•°æ®éªŒè¯å’Œæ¸…æ´—
3. å¤šç®—æ³•é˜¶æ®µä½ç‚¹æ£€æµ‹ï¼ˆå…¨å±€æœ€ä½ç‚¹ã€æ»‘åŠ¨çª—å£ã€ä»·æ ¼åˆ†ä½æ•°ã€æŠ€æœ¯æŒ‡æ ‡ï¼‰
4. é«˜è´¨é‡å›¾è¡¨ç»˜åˆ¶ï¼ˆKçº¿å›¾ã€é˜¶æ®µä½ç‚¹çº¿ã€ç™¾åˆ†æ¯”æ¶¨å¹…çº¿ï¼‰
5. å¤šçº¿ç¨‹æ‰¹é‡å¤„ç†
6. å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
7. å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ

ä½¿ç”¨æ–¹æ³•ï¼š
    # å¤„ç†å•åªè‚¡ç¥¨
    python draw_lines_unified.py --stock 002895
    
    # æ‰¹é‡å¤„ç†æ‰€æœ‰è‚¡ç¥¨
    python draw_lines_unified.py --all
    
    # æŒ‡å®šè¾“å‡ºç›®å½•å’Œçº¿ç¨‹æ•°
    python draw_lines_unified.py --all --output drawLineRes --workers 4
    
    # ä»æŒ‡å®šæ•°æ®ç›®å½•è¯»å–
    python draw_lines_unified.py --all --data-dir ../data
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union
import logging
from datetime import datetime
import glob
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
from scipy.signal import argrelextrema

# è®¾ç½®matplotlibåç«¯å’Œå­—ä½“
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯ï¼Œæ”¯æŒå¤šçº¿ç¨‹
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# å­—ä½“é…ç½® - ä½¿ç”¨macOSç³»ç»Ÿæ”¯æŒçš„ä¸­æ–‡å­—ä½“
plt.rcParams['font.family'] = ['Heiti TC', 'PingFang HK', 'Arial Unicode MS', 'Arial', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] [%(threadName)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('draw_lines_unified.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# çº¿ç¨‹é”
progress_lock = threading.Lock()
matplotlib_lock = threading.Lock()  # ç”¨äºmatplotlibæ“ä½œçš„çº¿ç¨‹å®‰å…¨


class UnifiedLineDrawer:
    """ç»Ÿä¸€ç‰ˆç”»çº¿å™¨ - æ•´åˆæ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self, config_file: str = "lineConfig.json"):
        """åˆå§‹åŒ–ç»Ÿä¸€ç”»çº¿å™¨"""
        self.config_file = config_file
        self.percent_list = self._load_config()
        self.stock_info = self._load_stock_info()
        self.processed_count = 0
        self.total_count = 0
        logger.info(f"âœ… ç»Ÿä¸€ç”»çº¿å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š åŠ è½½{len(self.percent_list)}ä¸ªç™¾åˆ†æ¯”é…ç½®: {self.percent_list}")
        logger.info(f"ğŸ“ˆ åŠ è½½{len(self.stock_info)}åªè‚¡ç¥¨ä¿¡æ¯")
    
    def _load_config(self) -> List[str]:
        """åŠ è½½é…ç½®æ–‡ä»¶ä¸­çš„ç™¾åˆ†æ¯”æ•°æ®å’ŒZigZagå‚æ•°"""
        try:
            config_path = Path(self.config_file)
            if not config_path.exists():
                # å°è¯•åœ¨ä¸Šçº§ç›®å½•æŸ¥æ‰¾
                config_path = Path("..") / self.config_file
                if not config_path.exists():
                    logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                    default_percents = ["3%", "16%", "25%", "34%", "50%", "67%", "128%", "228%", "247%", "323%", "457%", "589%", "636%", "770%", "823%", "935%"]
                    self.zigzag_period = 20
                    self.zigzag_threshold = 0.05
                    return default_percents
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                percent_dic = config.get('percent_dic', [])
                self.zigzag_period = config.get('zigzag_period', 20)
                self.zigzag_threshold = config.get('zigzag_threshold', 0.05)
                logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                logger.info(f"ğŸ”§ ZigZagå‘¨æœŸ: {self.zigzag_period}, é˜ˆå€¼: {self.zigzag_threshold}")
                return percent_dic
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            default_percents = ["3%", "16%", "25%", "34%", "50%", "67%", "128%", "228%", "247%", "323%", "457%", "589%", "636%", "770%", "823%", "935%"]
            self.zigzag_period = 20
            self.zigzag_threshold = 0.05
            logger.info(f"ä½¿ç”¨é»˜è®¤é…ç½®")
            return default_percents
    
    def _load_stock_info(self) -> Dict[str, Dict[str, str]]:
        """ä»stocklist.csvåŠ è½½è‚¡ç¥¨ä¿¡æ¯ï¼ˆåç§°å’Œè¡Œä¸šï¼‰"""
        stock_info = {}
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„stocklist.csvè·¯å¾„
            possible_paths = ["../stocklist.csv", "stocklist.csv", "./stocklist.csv"]
            stocklist_file = None
            
            for path in possible_paths:
                if os.path.exists(path):
                    stocklist_file = path
                    break
            
            if stocklist_file:
                # ä»stocklist.csvåŠ è½½è‚¡ç¥¨ä¿¡æ¯
                try:
                    df = pd.read_csv(stocklist_file)
                    logger.info(f"ğŸ“ ä»{stocklist_file}åŠ è½½è‚¡ç¥¨ä¿¡æ¯")
                    
                    if 'symbol' in df.columns and 'name' in df.columns and 'industry' in df.columns:
                        for _, row in df.iterrows():
                            code = str(row['symbol']).zfill(6)
                            name = str(row['name'])
                            industry = str(row['industry']) if pd.notna(row['industry']) else "æœªçŸ¥è¡Œä¸š"
                            stock_info[code] = {'name': name, 'industry': industry}
                        
                        logger.info(f"âœ… ä»stocklist.csvåŠ è½½è‚¡ç¥¨ä¿¡æ¯å®Œæˆï¼Œå…±{len(stock_info)}åªè‚¡ç¥¨")
                        return stock_info
                    else:
                        logger.warning(f"âš ï¸ stocklist.csvç¼ºå°‘å¿…è¦å­—æ®µ: symbol, name, industry")
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–stocklist.csvå¤±è´¥: {e}")
            
            # å¦‚æœstocklist.csvä¸å¯ç”¨ï¼Œå›é€€åˆ°ä»resByFilterç›®å½•åŠ è½½
            logger.info("ğŸ“ stocklist.csvä¸å¯ç”¨ï¼Œå›é€€åˆ°ä»resByFilterç›®å½•åŠ è½½è‚¡ç¥¨ä¿¡æ¯")
            possible_paths = ["../resByFilter", "resByFilter", "./resByFilter"]
            res_dir = None
            
            for path in possible_paths:
                if os.path.exists(path):
                    res_dir = path
                    break
            
            if not res_dir:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°resByFilterç›®å½•ï¼Œå°è¯•çš„è·¯å¾„: {possible_paths}")
                return stock_info
            
            csv_files = glob.glob(os.path.join(res_dir, "*.csv"))
            logger.info(f"ğŸ“ åœ¨{res_dir}ä¸­æ‰¾åˆ°{len(csv_files)}ä¸ªCSVæ–‡ä»¶")
            
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file)
                    if 'code' in df.columns and 'name' in df.columns:
                        for _, row in df.iterrows():
                            code = str(row['code']).zfill(6)
                            name = str(row['name'])
                            stock_info[code] = {'name': name, 'industry': "æœªçŸ¥è¡Œä¸š"}
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {csv_file}: {e}")
            
            logger.info(f"âœ… åŠ è½½è‚¡ç¥¨ä¿¡æ¯å®Œæˆï¼Œå…±{len(stock_info)}åªè‚¡ç¥¨")
            return stock_info
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
            return stock_info
    
    def get_stock_list(self, data_dir: str = "../data") -> List[Tuple[str, str, str]]:
        """è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œè¿”å›(code, name, industry)"""
        stock_list = []
        
        # å¦‚æœæœ‰è‚¡ç¥¨ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨
        if self.stock_info:
            for code, info in self.stock_info.items():
                name = info.get('name', code)
                industry = info.get('industry', 'æœªçŸ¥è¡Œä¸š')
                stock_list.append((code, name, industry))
            logger.info(f"ğŸ“‹ ä»è‚¡ç¥¨ä¿¡æ¯è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_list)}åª")
            return stock_list
        
        # å¦åˆ™ä»æ•°æ®ç›®å½•è·å–
        try:
            data_path = Path(data_dir)
            if not data_path.exists():
                logger.error(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
                return stock_list
            
            csv_files = list(data_path.glob("*.csv"))
            for csv_file in csv_files:
                code = csv_file.stem
                # å°è¯•ä»æ–‡ä»¶åæ¨æ–­è‚¡ç¥¨åç§°ï¼Œæˆ–ä½¿ç”¨ä»£ç ä½œä¸ºåç§°
                name = code
                industry = "æœªçŸ¥è¡Œä¸š"
                stock_list.append((code, name, industry))
            
            logger.info(f"ğŸ“‹ ä»æ•°æ®ç›®å½•è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_list)}åª")
            return stock_list
            
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return stock_list
    
    def validate_and_load_data(self, stock_code: str, data_dir: str) -> Optional[pd.DataFrame]:
        """éªŒè¯å¹¶åŠ è½½è‚¡ç¥¨æ•°æ®"""
        try:
            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼ˆè¡¥é›¶åˆ°6ä½ï¼‰
            normalized_code = str(stock_code).zfill(6)
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            data_path = Path(data_dir)
            csv_file = data_path / f"{normalized_code}.csv"
            
            if not csv_file.exists():
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
                return None
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = csv_file.stat().st_size
            if file_size < 1000:  # å°äº1KBçš„æ–‡ä»¶å¯èƒ½æœ‰é—®é¢˜
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶è¿‡å°: {csv_file} ({file_size} bytes)")
                return None
            
            # è¯»å–æ•°æ®
            df = pd.read_csv(csv_file)
            
            # éªŒè¯å¿…è¦åˆ—
            required_columns = ['date', 'open', 'high', 'low', 'close']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦åˆ— {missing_columns}: {stock_code}")
                return None
            
            # æ£€æŸ¥æ•°æ®è¡Œæ•°
            if len(df) < 100:  # è‡³å°‘éœ€è¦100å¤©çš„æ•°æ®
                logger.warning(f"âš ï¸ æ•°æ®è¡Œæ•°ä¸è¶³: {stock_code} ({len(df)} rows)")
                return None
            
            # æ•°æ®æ¸…æ´—
            df = df.dropna(subset=required_columns)
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
            
            # éªŒè¯ä»·æ ¼æ•°æ®åˆç†æ€§
            price_columns = ['open', 'high', 'low', 'close']
            for col in price_columns:
                if (df[col] <= 0).any():
                    logger.warning(f"âš ï¸ å‘ç°éæ­£ä»·æ ¼æ•°æ®: {stock_code}")
                    df = df[df[col] > 0]
            
            # éªŒè¯é«˜ä½ä»·å…³ç³»
            invalid_rows = (df['high'] < df['low']) | (df['high'] < df['open']) | (df['high'] < df['close']) | \
                          (df['low'] > df['open']) | (df['low'] > df['close'])
            if invalid_rows.any():
                logger.warning(f"âš ï¸ å‘ç°ä»·æ ¼é€»è¾‘é”™è¯¯: {stock_code}")
                df = df[~invalid_rows]
            
            if len(df) < 50:  # æ¸…æ´—åæ•°æ®å¤ªå°‘
                logger.warning(f"âš ï¸ æ¸…æ´—åæ•°æ®ä¸è¶³: {stock_code} ({len(df)} rows)")
                return None
            
            logger.debug(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: {stock_code} ({len(df)} rows)")
            return df
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ {stock_code}: {e}")
            return None
    
    def find_stage_lows_unified(self, df: pd.DataFrame) -> List[Tuple[int, float, str]]:
        """ç»Ÿä¸€ç‰ˆé˜¶æ®µä½ç‚¹æ£€æµ‹ - åŸºäºé€šè¾¾ä¿¡TROUGHBARSç®—æ³•"""
        try:
            # ä»é…ç½®æ–‡ä»¶è¯»å–zigzagå‚æ•°
            zigzag_period = getattr(self, 'zigzag_period', 20)
            zigzag_threshold = getattr(self, 'zigzag_threshold', 0.05)
            
            if len(df) < zigzag_period:
                logger.warning(f"âš ï¸ æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{zigzag_period}ä¸ªæ•°æ®ç‚¹")
                return []
            
            logger.debug(f"ğŸ” å¼€å§‹TROUGHBARSé˜¶æ®µä½ç‚¹æ£€æµ‹: å‘¨æœŸ={zigzag_period}, é˜ˆå€¼={zigzag_threshold}")
            
            # å®ç°é€šè¾¾ä¿¡TROUGHBARSç®—æ³•
            def troughbars(low_prices: np.ndarray, period: int) -> np.ndarray:
                """é€šè¾¾ä¿¡TROUGHBARSå‡½æ•°å®ç°"""
                result = np.zeros(len(low_prices), dtype=int)
                
                for i in range(len(low_prices)):
                    start_idx = max(0, i - period + 1)
                    end_idx = i + 1
                    
                    if end_idx - start_idx < period:
                        result[i] = -1
                        continue
                    
                    window_lows = low_prices[start_idx:end_idx]
                    min_idx_in_window = np.argmin(window_lows)
                    actual_min_idx = start_idx + min_idx_in_window
                    distance = i - actual_min_idx
                    result[i] = distance
                
                return result
            
            # å®ç°é€šè¾¾ä¿¡REFå’ŒBARSLASTç®—æ³•
            def find_lowest_price_with_barslast(low_prices: np.ndarray, trough_distances: np.ndarray) -> Tuple[int, float]:
                """å®ç°: ä½ä»·1:=REF(L,BARSLAST(æœ€ä½APP=0))"""
                zero_distance_indices = np.where(trough_distances == 0)[0]
                
                if len(zero_distance_indices) == 0:
                    min_idx = np.argmin(low_prices)
                    logger.debug(f"âš ï¸ æœªæ‰¾åˆ°è·ç¦»ä¸º0çš„ç‚¹ï¼Œä½¿ç”¨å…¨å±€æœ€ä½ç‚¹: ç´¢å¼•={min_idx}, ä»·æ ¼={low_prices[min_idx]:.2f}")
                    return min_idx, low_prices[min_idx]
                
                # æ‰¾åˆ°å†å²æœ€é«˜ä»·ä½œä¸ºå‚è€ƒ
                max_high_idx = np.argmax(df['high'].values)
                max_high_price = df.loc[max_high_idx, 'high']
                
                # è®¡ç®—æ¯ä¸ªè·ç¦»ä¸º0çš„ç‚¹çš„ç»¼åˆè¯„åˆ†ï¼ˆè·Œå¹… + æ—¶é—´æƒé‡ï¼‰
                best_idx = zero_distance_indices[0]
                best_score = 0
                
                for idx in zero_distance_indices:
                    if idx > max_high_idx:  # åªè€ƒè™‘å±±å³°åçš„ä½ç‚¹
                        decline = (max_high_price - low_prices[idx]) / max_high_price * 100
                        # æ—¶é—´æƒé‡ï¼šæ›´è¿‘æœŸçš„ä½ç‚¹è·å¾—æ›´é«˜æƒé‡
                        time_weight = (idx - max_high_idx) / (len(low_prices) - max_high_idx) * 100  # æ—¶é—´æƒé‡0-100
                        score = decline + time_weight  # ç»¼åˆè¯„åˆ†
                        
                        if score > best_score:
                            best_score = score
                            best_idx = idx
                
                # å¦‚æœæ‰¾åˆ°äº†å±±å³°åçš„ä½ç‚¹ï¼Œä½¿ç”¨å®ƒ
                if best_score > 0:
                    decline = (max_high_price - low_prices[best_idx]) / max_high_price * 100
                    logger.debug(f"âœ… æ‰¾åˆ°å±±å³°åæœ€ä½³ä½ç‚¹: ç´¢å¼•={best_idx}, ä»·æ ¼={low_prices[best_idx]:.2f}, è·Œå¹…={decline:.2f}%, è¯„åˆ†={best_score:.2f}")
                    return best_idx, low_prices[best_idx]
                else:
                    # å¦‚æœæ²¡æœ‰å±±å³°åçš„ä½ç‚¹ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªè·ç¦»ä¸º0çš„ç‚¹
                    last_zero_idx = zero_distance_indices[-1]
                    last_zero_price = low_prices[last_zero_idx]
                    logger.debug(f"âœ… ä½¿ç”¨æœ€è¿‘ä¸€æ¬¡æœ€ä½ç‚¹: ç´¢å¼•={last_zero_idx}, ä»·æ ¼={last_zero_price:.2f}")
                    return last_zero_idx, last_zero_price
            
            # æ‰§è¡ŒTROUGHBARSç®—æ³•
            low_prices = df['low'].values
            trough_distances = troughbars(low_prices, zigzag_period)
            
            # æ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡æœ€ä½ç‚¹
            final_low_idx, final_low_price = find_lowest_price_with_barslast(low_prices, trough_distances)
            final_low_date = df.loc[final_low_idx, "date"]
            
            # è®¡ç®—ä»å†å²æœ€é«˜ä»·çš„è·Œå¹…
            max_high_idx = df['high'].idxmax()
            max_high_price = df.loc[max_high_idx, 'high']
            actual_decline = (max_high_price - final_low_price) / max_high_price * 100
            
            logger.debug(f"âœ… TROUGHBARSæ£€æµ‹åˆ°é˜¶æ®µä½ç‚¹: æ—¥æœŸ={final_low_date}, "
                       f"ä»·æ ¼={final_low_price:.2f}, è·Œå¹…={actual_decline:.2f}%")
            
            # æ ¼å¼åŒ–æ—¥æœŸ
            if hasattr(final_low_date, 'strftime'):
                final_low_date_str = final_low_date.strftime("%Y-%m-%d")
            else:
                final_low_date_str = str(final_low_date)
            
            # è¿”å›å•ä¸€ä½ç‚¹
            stage_lows = [(final_low_idx, final_low_price, final_low_date_str)]
            
            logger.debug(f"âœ… æœ€ç»ˆé˜¶æ®µä½ç‚¹: æ—¥æœŸ={final_low_date_str}, ä»·æ ¼={final_low_price:.2f}")
            return stage_lows
            
        except Exception as e:
            logger.error(f"âŒ TROUGHBARSé˜¶æ®µä½ç‚¹æ£€æµ‹å¤±è´¥: {e}")
            # å¤‡é€‰æ–¹æ¡ˆï¼šè¿”å›å…¨å±€æœ€ä½ç‚¹
            try:
                global_min_idx = df['low'].idxmin()
                global_min_price = df.loc[global_min_idx, 'low']
                global_min_date = df.loc[global_min_idx, 'date'].strftime('%Y-%m-%d')
                return [(global_min_idx, global_min_price, global_min_date)]
            except:
                return []
    
    def create_unified_chart(self, stock_code: str, stock_name: str, df: pd.DataFrame, 
                           stage_lows: List[Tuple[int, float, str]], output_file: str) -> bool:
        """åˆ›å»ºç»Ÿä¸€ç‰ˆé«˜è´¨é‡å›¾è¡¨ - ä½¿ç”¨mplfinanceç»˜åˆ¶ä¸“ä¸šKçº¿å›¾"""
        try:
            # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿matplotlibæ“ä½œçš„çº¿ç¨‹å®‰å…¨
            with matplotlib_lock:
                # åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼Œç¡®ä¿matplotlibæ“ä½œçš„çº¿ç¨‹å®‰å…¨
                import matplotlib
                matplotlib.use('Agg')  # ç¡®ä¿ä½¿ç”¨éäº¤äº’å¼åç«¯
                
                import mplfinance as mpf
                
                # è·å–æœ€ä½ç‚¹ä½ç½®ï¼Œåªæ˜¾ç¤ºä»æœ€ä½ç‚¹å¼€å§‹å¾€åçš„æ•°æ®
                if stage_lows:
                    lowest_idx, _, _ = stage_lows[0]  # è·å–æœ€ä½ç‚¹çš„ç´¢å¼•
                    # æˆªå–ä»æœ€ä½ç‚¹å¼€å§‹çš„æ•°æ®
                    df_display = df.iloc[lowest_idx:].copy()
                    # ä¸è¦é‡ç½®ç´¢å¼•ï¼Œä¿æŒåŸå§‹ç´¢å¼•
                else:
                    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä½ç‚¹ï¼Œæ˜¾ç¤ºå…¨éƒ¨æ•°æ®
                    df_display = df.copy()
                
                # å‡†å¤‡mplfinanceéœ€è¦çš„æ•°æ®æ ¼å¼
                df_mpf = df_display.copy()
                df_mpf['date'] = pd.to_datetime(df_mpf['date'])
                df_mpf.set_index('date', inplace=True)
                
                # ç¡®ä¿åˆ—åç¬¦åˆmplfinanceè¦æ±‚
                df_mpf = df_mpf[['open', 'high', 'low', 'close']].copy()
                
                # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
                if df_mpf.empty:
                    logger.warning(f"âš ï¸ å¤„ç†åçš„æ•°æ®ä¸ºç©º: {stock_code}")
                    return False
                
                logger.debug(f"ğŸ“Š mplfinanceæ•°æ®: {len(df_mpf)} è¡Œ, åˆ—: {list(df_mpf.columns)}")
                
                # å‡†å¤‡é¢å¤–çš„ç»˜å›¾å…ƒç´ 
                additional_plots = []
                
                # 1. æ·»åŠ é˜¶æ®µä½ç‚¹æ°´å¹³çº¿
                for i, (idx, price, date_str) in enumerate(stage_lows):
                    # åˆ›å»ºæ°´å¹³çº¿æ•°æ®
                    hline_data = [price] * len(df_mpf)
                    additional_plots.append(mpf.make_addplot(hline_data, color='blue', linestyle='-', width=2, alpha=0.8))
                
                # 2. æ·»åŠ ç™¾åˆ†æ¯”æ¶¨å¹…çº¿
                if stage_lows:
                    base_price = min(price for _, price, _ in stage_lows)  # ä½¿ç”¨æœ€ä½ä»·ä½œä¸ºåŸºå‡†
                    max_price = df_mpf['high'].max()
                    
                    # å…ˆç”»åŸæœ‰çš„ç™¾åˆ†æ¯”çº¿ï¼Œæ‰¾å‡ºKçº¿è¦†ç›–èŒƒå›´å†…æœ€ä¸Šæ–¹çš„ç™¾åˆ†æ¯”çº¿
                    visible_percent_lines = []
                    for i, percent_str in enumerate(self.percent_list):
                        try:
                            percent = float(percent_str.rstrip('%')) / 100
                            target_price = base_price * (1 + percent)
                            
                            # æ‰€æœ‰ç™¾åˆ†æ¯”çº¿éƒ½é™åˆ¶åœ¨Kçº¿æ–¹æ¡†å†…ï¼ˆæœ€é«˜ä»·çš„100%ä»¥å†…ï¼‰
                            if target_price <= max_price:  # é™åˆ¶åœ¨Kçº¿æœ€é«˜ä»·ä»¥å†…
                                visible_percent_lines.append((percent_str, target_price))
                                # åˆ›å»ºæ°´å¹³çº¿æ•°æ®
                                hline_data = [target_price] * len(df_mpf)
                                additional_plots.append(mpf.make_addplot(hline_data, color='hotpink', linestyle='--', width=3, alpha=0.8))
                        except (ValueError, TypeError):
                            continue
                
                
                # è·å–è¡Œä¸šä¿¡æ¯
                industry = ""
                if stock_code in self.stock_info:
                    industry = self.stock_info[stock_code].get('industry', '')
                
                # æ„å»ºæ ‡é¢˜
                title_parts = [stock_code, stock_name]
                if industry and industry != "æœªçŸ¥è¡Œä¸š":
                    title_parts.append(f"({industry})")
                title = " ".join(title_parts) + " - Stage Low Points Analysis"
                
                # è®¾ç½®ä¸­æ–‡å­—ä½“
                import matplotlib.pyplot as plt
                plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
                plt.rcParams['axes.unicode_minus'] = False
                
                # è®¾ç½®mplfinanceæ ·å¼
                style = mpf.make_mpf_style(
                    base_mpf_style='charles',
                    gridstyle='-',
                    gridcolor='lightgray',
                    y_on_right=True,
                    facecolor='white',
                    edgecolor='black',
                    figcolor='white',
                    rc={'font.size': 12, 'axes.titlesize': 20, 'axes.labelsize': 14, 
                        'font.sans-serif': ['SimHei', 'Arial Unicode MS', 'DejaVu Sans'],
                        'axes.unicode_minus': False}
                )
                
                # åˆ›å»ºå›¾è¡¨
                fig, axes = mpf.plot(
                    df_mpf,
                    type='candle',
                    style=style,
                    title=title,
                    ylabel='Price',
                    volume=False,
                    addplot=additional_plots if additional_plots else None,
                    figsize=(20, 12),
                    tight_layout=True,
                    returnfig=True,
                    panel_ratios=(1,),  # åªæ˜¾ç¤ºä¸»å›¾
                    show_nontrading=False,  # ä¸æ˜¾ç¤ºéäº¤æ˜“æ—¥
                    datetime_format='%Y-%m',  # æ—¥æœŸæ ¼å¼
                    xrotation=45  # Xè½´æ ‡ç­¾æ—‹è½¬
                )
                
                # æ·»åŠ ä»·æ ¼æ ‡æ³¨
                ax = axes[0]  # è·å–ä¸»å›¾è½´
                
                # æ ‡æ³¨é˜¶æ®µä½ç‚¹ä»·æ ¼
                for i, (idx, price, date_str) in enumerate(stage_lows):
                    ax.text(1.02, price, f'{price:.2f}', 
                           fontsize=16, color='blue', fontweight='bold',
                           bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8),
                           transform=ax.get_yaxis_transform(), ha='left', va='center')
                
                # æ ‡æ³¨ç™¾åˆ†æ¯”æ¶¨å¹…çº¿
                if stage_lows:
                    base_price = min(price for _, price, _ in stage_lows)
                    max_price = df_mpf['high'].max()
                    
                    # æ ‡æ³¨Kçº¿è¦†ç›–èŒƒå›´å†…çš„ç™¾åˆ†æ¯”çº¿
                    for percent_str in self.percent_list:
                        try:
                            percent = float(percent_str.rstrip('%')) / 100
                            target_price = base_price * (1 + percent)
                            
                            if target_price <= max_price:  # é™åˆ¶åœ¨Kçº¿æœ€é«˜ä»·ä»¥å†…
                                ax.text(1.02, target_price, f'+{percent_str}', 
                                       fontsize=18, color='#8B7355', fontweight='bold',
                                       bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.9, edgecolor='#8B7355', linewidth=2),
                                       transform=ax.get_yaxis_transform(), ha='left', va='center')
                        except (ValueError, TypeError):
                            continue
                    
                
                # é‡æ–°ä¿å­˜å¸¦æ ‡æ³¨çš„å›¾è¡¨
                plt.savefig(output_file, dpi=200, bbox_inches='tight', facecolor='white', edgecolor='none')
                
                # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
                plt.close(fig)
                
                # è°ƒæ•´å›¾ç‰‡å°ºå¯¸åˆ°ç²¾ç¡®çš„ç›®æ ‡å°ºå¯¸
                try:
                    from PIL import Image
                    with Image.open(output_file) as img:
                        # è°ƒæ•´åˆ°ç²¾ç¡®çš„ç›®æ ‡å°ºå¯¸ (3991 x 2392)
                        target_width = 3991
                        target_height = 2392
                        resized_img = img.resize((target_width, target_height), Image.Resampling.LANCZOS)
                        resized_img.save(output_file, 'PNG', quality=95)
                        logger.debug(f"âœ… å›¾ç‰‡å°ºå¯¸å·²è°ƒæ•´: {target_width}x{target_height}")
                except ImportError:
                    logger.warning("âš ï¸ PILæœªå®‰è£…ï¼Œæ— æ³•è°ƒæ•´å›¾ç‰‡å°ºå¯¸")
                except Exception as e:
                    logger.warning(f"âš ï¸ è°ƒæ•´å›¾ç‰‡å°ºå¯¸å¤±è´¥: {e}")
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸç”Ÿæˆ
                if os.path.exists(output_file):
                    file_size = os.path.getsize(output_file)
                    if file_size > 1000:  # è‡³å°‘1KB
                        logger.debug(f"âœ… å›¾è¡¨æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {output_file} ({file_size} bytes)")
                        return True
                    else:
                        logger.warning(f"âš ï¸ ç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶è¿‡å°: {output_file} ({file_size} bytes)")
                        return False
                else:
                    logger.error(f"âŒ å›¾è¡¨æ–‡ä»¶æœªç”Ÿæˆ: {output_file}")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ å›¾è¡¨åˆ›å»ºå¤±è´¥ {stock_code}: {type(e).__name__}: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
        finally:
            # ç¡®ä¿é‡Šæ”¾matplotlibèµ„æº
            try:
                plt.close('all')  # å…³é—­æ‰€æœ‰å›¾å½¢
            except Exception as cleanup_error:
                logger.debug(f"èµ„æºæ¸…ç†å¼‚å¸¸: {cleanup_error}")
                pass
    
    def process_stock_list(self, stock_list: List[Tuple[str, str, str, str]], 
                          output_dir: str = None, data_dir: str = "../data", workers: int = 4):
        """å¤„ç†æŒ‡å®šçš„è‚¡ç¥¨åˆ—è¡¨"""
        # å¦‚æœæœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨å¸¦æ—¥æœŸçš„é»˜è®¤ç›®å½•
        if output_dir is None:
            current_date = datetime.now().strftime('%Y%m%d')
            output_dir = f'{current_date}-drawLineRes'
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨")
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"ğŸ§µ çº¿ç¨‹æ•°: {workers}")
        
        if not stock_list:
            logger.error("âŒ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
            return
        
        self.total_count = len(stock_list)
        self.processed_count = 0
        
        logger.info(f"ğŸ“Š å¾…å¤„ç†è‚¡ç¥¨æ•°é‡: {self.total_count}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # å¤šçº¿ç¨‹å¤„ç†
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_stock = {
                executor.submit(self._process_single_stock, code, name, output_dir, data_dir, file_prefix): (code, name, industry, file_prefix)
                for code, name, industry, file_prefix in stock_list
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_stock):
                result = future.result()
                results.append(result)
        
        # ç»Ÿè®¡ç»“æœ
        total_time = time.time() - start_time
        success_count = sum(1 for r in results if r['success'])
        failed_count = len(results) - success_count
        
        logger.info(f"ğŸ‰ è‚¡ç¥¨åˆ—è¡¨å¤„ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»è®¡: {len(results)}åªè‚¡ç¥¨")
        logger.info(f"âœ… æˆåŠŸ: {success_count}åª")
        logger.info(f"âŒ å¤±è´¥: {failed_count}åª")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {len(results)/total_time:.2f}åª/ç§’")
        
        # ä¿å­˜å¤„ç†ç»“æœ
        results_file = os.path.join(output_dir, "processing_results.json")
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“„ å¤„ç†ç»“æœå·²ä¿å­˜: {results_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„è‚¡ç¥¨
        failed_stocks = [r for r in results if not r['success']]
        if failed_stocks:
            logger.warning(f"âš ï¸ å¤±è´¥çš„è‚¡ç¥¨:")
            for r in failed_stocks[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                logger.warning(f"   {r['stock_code']} {r['stock_name']}: {r['error']}")
            if len(failed_stocks) > 10:
                logger.warning(f"   ... è¿˜æœ‰{len(failed_stocks)-10}åªè‚¡ç¥¨å¤±è´¥")

    def _process_single_stock(self, stock_code: str, stock_name: str, 
                           output_dir: str, data_dir: str, file_prefix: str = "") -> dict:
        """å¤„ç†å•åªè‚¡ç¥¨ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        start_time = time.time()
        result = {
            'stock_code': stock_code,
            'stock_name': stock_name,
            'success': False,
            'elapsed_time': 0,
            'error': None,
            'stage_lows_count': 0
        }
        
        try:
            # 1. éªŒè¯å¹¶åŠ è½½æ•°æ®
            df = self.validate_and_load_data(stock_code, data_dir)
            if df is None:
                result['error'] = "æ•°æ®åŠ è½½å¤±è´¥"
                return result
            
            # 2. æ£€æµ‹é˜¶æ®µä½ç‚¹
            stage_lows = self.find_stage_lows_unified(df)
            if not stage_lows:
                result['error'] = "æœªæ£€æµ‹åˆ°é˜¶æ®µä½ç‚¹"
                return result
            
            result['stage_lows_count'] = len(stage_lows)
            
            # 3. åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # 4. ç”Ÿæˆå›¾è¡¨
            # æ ¹æ®æ–‡ä»¶å‰ç¼€ç”Ÿæˆå¸¦å‰ç¼€çš„æ–‡ä»¶å
            if file_prefix and file_prefix != "UNKNOWN":
                output_file = os.path.join(output_dir, f"{file_prefix}_{stock_code}_{stock_name}.png")
            else:
                output_file = os.path.join(output_dir, f"{stock_code}_{stock_name}.png")
            success = self.create_unified_chart(stock_code, stock_name, df, stage_lows, output_file)
            
            if success:
                result['success'] = True
                
                # æ›´æ–°è¿›åº¦
                with progress_lock:
                    self.processed_count += 1
                    logger.info(f"âœ… [{self.processed_count}/{self.total_count}] {stock_code} {stock_name} - {len(stage_lows)}ä¸ªä½ç‚¹")
            else:
                result['error'] = "å›¾è¡¨åˆ›å»ºå¤±è´¥"
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"âŒ å¤„ç†è‚¡ç¥¨å¤±è´¥ {stock_code}: {e}")
        
        finally:
            result['elapsed_time'] = time.time() - start_time
        
        return result


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="åŸºç¡€å±‚ç”»çº¿è„šæœ¬ - è¯»å–resByFilterä¸­çš„è‚¡ç¥¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # é»˜è®¤è¡Œä¸ºï¼šè¯»å–å½“å‰æ—¥æœŸçš„resByFilterä¸­çš„è‚¡ç¥¨
  python draw_lines_unified.py
  
  # è¯»å–æŒ‡å®šæ—¥æœŸçš„resByFilterä¸­çš„è‚¡ç¥¨
  python draw_lines_unified.py --date 2025-01-15
  
  # æŒ‡å®šçº¿ç¨‹æ•°
  python draw_lines_unified.py --workers 6
        """
    )
    
    # ç”Ÿæˆå¸¦æ—¥æœŸçš„é»˜è®¤è¾“å‡ºç›®å½•
    current_date = datetime.now().strftime('%Y%m%d')
    
    # å‚æ•°
    parser.add_argument('--date', type=str, 
                       help='æ—¥æœŸå‚æ•°ï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œç”¨äºæ„å»ºresByFilterç›®å½•')
    parser.add_argument('--workers', type=int, default=4,
                       help='å¹¶å‘å¤„ç†çš„çº¿ç¨‹æ•° (é»˜è®¤: 4)')
    
    args = parser.parse_args()
    
    # å¤„ç†æ—¥æœŸå‚æ•°
    if args.date:
        try:
            # éªŒè¯æ—¥æœŸæ ¼å¼å¹¶è½¬æ¢
            date_obj = datetime.strptime(args.date, '%Y-%m-%d')
            date_str = date_obj.strftime('%Y%m%d')
        except ValueError:
            logger.error(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {args.date}ï¼Œè¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼")
            sys.exit(1)
    else:
        date_str = current_date
    
    # åˆ›å»ºç»Ÿä¸€ç”»çº¿å™¨
    drawer = UnifiedLineDrawer()
    
    # è¯»å–æŒ‡å®šæ—¥æœŸçš„resByFilterä¸­çš„è‚¡ç¥¨
    filter_dir = f"../{date_str}-resByFilter"
    if not os.path.exists(filter_dir):
        logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {filter_dir}")
        logger.info(f"ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿å­˜åœ¨ {filter_dir} ç›®å½•")
        sys.exit(1)
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶ï¼ˆPDIå’ŒADXç»“æœæ–‡ä»¶ï¼‰
    csv_files = glob.glob(os.path.join(filter_dir, "*.csv"))
    if not csv_files:
        logger.error(f"âŒ åœ¨ç›®å½• {filter_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        logger.info(f"ğŸ’¡ æç¤ºï¼šè¯·åœ¨ {filter_dir} ç›®å½•ä¸­æ”¾ç½®è‚¡ç¥¨åˆ—è¡¨CSVæ–‡ä»¶")
        sys.exit(1)
    
    logger.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    # è¯»å–æ‰€æœ‰CSVæ–‡ä»¶ä¸­çš„è‚¡ç¥¨ï¼Œå¹¶å»é‡
    all_stocks = {}  # ä½¿ç”¨å­—å…¸å»é‡ï¼Œkeyä¸ºè‚¡ç¥¨ä»£ç 
    
    for file_path in csv_files:
        logger.info(f"ğŸ“„ è¯»å–æ–‡ä»¶: {file_path}")
        try:
            import pandas as pd
            df = pd.read_csv(file_path)
            
            # ä»æ–‡ä»¶åæå–å‰ç¼€ç±»å‹ï¼ˆä¿æŒåŸå§‹æ•°å­—ï¼‰
            file_name = os.path.basename(file_path)
            file_prefix = ""
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ADXæˆ–PDIåçš„æ•°å­—
            import re
            
            # å®šä¹‰åŒ¹é…æ¨¡å¼
            patterns = [
                (r'^ADX(\d+)', 'ADX'),  # åŒ¹é…å¼€å¤´çš„ADX
                (r'^PDI(\d+)', 'PDI'),  # åŒ¹é…å¼€å¤´çš„PDI
                (r'ADX(\d+)', 'ADX'),    # åŒ¹é…ä»»æ„ä½ç½®çš„ADX
                (r'PDI(\d+)', 'PDI')     # åŒ¹é…ä»»æ„ä½ç½®çš„PDI
            ]
            
            # æŒ‰ä¼˜å…ˆçº§å°è¯•åŒ¹é…
            for pattern, prefix_type in patterns:
                match = re.search(pattern, file_name.upper())
                if match:
                    file_prefix = f"{prefix_type}{match.group(1)}"
                    break
            
            logger.info(f"ğŸ“Š æ–‡ä»¶ç±»å‹: {file_prefix}")
            
            # ä»CSVæ–‡ä»¶ä¸­æå–è‚¡ç¥¨ä¿¡æ¯
            for _, row in df.iterrows():
                code = str(row.get('code', ''))
                name = str(row.get('name', code))
                industry = str(row.get('industry', 'æœªçŸ¥è¡Œä¸š'))
                
                # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼ˆè¡¥é›¶åˆ°6ä½ï¼‰
                if code:
                    normalized_code = code.zfill(6)
                    if normalized_code not in all_stocks:
                        # æ‰©å±•è‚¡ç¥¨ä¿¡æ¯ï¼ŒåŒ…å«æ–‡ä»¶å‰ç¼€
                        all_stocks[normalized_code] = (normalized_code, name, industry, file_prefix)
                        
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            continue
    
    if not all_stocks:
        logger.error(f"âŒ æœªè¯»å–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®")
        sys.exit(1)
    
    # è½¬æ¢ä¸ºåˆ—è¡¨
    stock_list = list(all_stocks.values())
    logger.info(f"ğŸ“‹ å»é‡åå…±æœ‰ {len(stock_list)} åªè‚¡ç¥¨")
    
    # ç”Ÿæˆè¾“å‡ºç›®å½•
    output_dir = f"{date_str}-drawLineRes"
    
    # æ‰¹é‡å¤„ç†è‚¡ç¥¨åˆ—è¡¨
    drawer.process_stock_list(stock_list, output_dir, "../data", args.workers)
    
    logger.info("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")


if __name__ == "__main__":
    main()