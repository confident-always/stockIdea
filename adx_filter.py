#!/usr/bin/env python3
"""
ADXè¿‡æ»¤è„šæœ¬
æ ¹æ®ADXfilteringconfigé…ç½®ï¼Œè¿‡æ»¤resç›®å½•ä¸­çš„ADXç»“æœ
æ£€æŸ¥è‚¡ç¥¨åœ¨ADXä¸Šç©¿æ—¥æœŸåçš„æ¶¨è·Œå¹…æ˜¯å¦ç¬¦åˆè®¾å®šèŒƒå›´
"""

import pandas as pd
import json
import os
import glob
import logging
import sys
from datetime import datetime, timedelta
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('adx_filter.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_config(config_file='configs.json'):
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: åŒ…å«ADXã€PDIå’Œstock_info_filteré…ç½®çš„å­—å…¸
    """
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # æå–ADXå’ŒPDIè¿‡æ»¤é…ç½®
        adx_config = config.get('ADXfilteringconfig', {})
        pdi_config = config.get('PDIfilteringconfig', {})
        stock_info_filter = config.get('stock_info_filter', {})
        
        return {
            'ADX': adx_config,
            'PDI': pdi_config,
            'stock_info_filter': stock_info_filter
        }
    except FileNotFoundError:
        logger.error(f"é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°")
        return {
            'ADX': {'active': False},
            'PDI': {'active': False},
            'stock_info_filter': {'active': False}
        }
    except json.JSONDecodeError as e:
        logger.error(f"é…ç½®æ–‡ä»¶ {config_file} æ ¼å¼é”™è¯¯: {e}")
        return {
            'ADX': {'active': False},
            'PDI': {'active': False},
            'stock_info_filter': {'active': False}
        }

def parse_percentage(pct_str):
    """è§£æç™¾åˆ†æ¯”å­—ç¬¦ä¸²ä¸ºæµ®ç‚¹æ•°ï¼Œæ”¯æŒnoneå€¼è¡¨ç¤ºä¸è¿‡æ»¤"""
    if pct_str is None or (isinstance(pct_str, str) and pct_str.lower() == 'none'):
        return None
    if isinstance(pct_str, str) and pct_str.endswith('%'):
        return float(pct_str[:-1]) / 100.0
    return 0.0

def parse_range_percent(range_str):
    """è§£æå½¢å¦‚ "A-B%" çš„åŒºé—´ç™¾åˆ†æ¯”é…ç½®ï¼Œè¿”å› (A, B) çš„æµ®ç‚¹åŒºé—´ï¼ˆæŒ‰æ¯”ä¾‹å€¼ï¼‰ï¼Œæˆ– None è¡¨ç¤ºä¸è¿‡æ»¤ã€‚
    ä¾‹å¦‚ï¼š"16-600%" => (0.16, 6.0)
    æ”¯æŒå¤§å°å†™çš„"none"è¿”å› Noneã€‚
    """
    if range_str is None:
        return None
    if isinstance(range_str, str) and range_str.strip().lower() == 'none':
        return None
    if isinstance(range_str, str) and range_str.endswith('%') and '-' in range_str:
        try:
            body = range_str[:-1]
            parts = body.split('-')
            if len(parts) != 2:
                return None
            low = float(parts[0]) / 100.0
            high = float(parts[1]) / 100.0
            return (low, high)
        except Exception:
            logger.warning(f"åŒºé—´ç™¾åˆ†æ¯”è§£æå¤±è´¥: {range_str}")
            return None
    logger.warning(f"æœªè¯†åˆ«çš„åŒºé—´é…ç½®: {range_str}")
    return None

def parse_number_range(range_str):
    """è§£ææ•°å€¼åŒºé—´é…ç½®ï¼Œè¿”å› (low, high) å…ƒç»„ï¼Œæˆ– None è¡¨ç¤ºä¸è¿‡æ»¤ã€‚
    ä¾‹å¦‚ï¼š"10-100" => (10.0, 100.0)
    æ”¯æŒå¤§å°å†™çš„"none"è¿”å› Noneã€‚
    """
    if range_str is None:
        return None
    if isinstance(range_str, str) and range_str.strip().lower() == 'none':
        return None
    if isinstance(range_str, str) and '-' in range_str:
        try:
            parts = range_str.split('-')
            if len(parts) != 2:
                return None
            low = float(parts[0])
            high = float(parts[1])
            return (low, high)
        except Exception:
            logger.warning(f"æ•°å€¼åŒºé—´è§£æå¤±è´¥: {range_str}")
            return None
    logger.warning(f"æœªè¯†åˆ«çš„æ•°å€¼åŒºé—´é…ç½®: {range_str}")
    return None

def parse_bool_or_none(value):
    """è§£æå¸ƒå°”å€¼æˆ–noneé…ç½®
    
    Args:
        value: é…ç½®å€¼ï¼Œå¯ä»¥æ˜¯boolã€strç­‰
    
    Returns:
        boolæˆ–None: Trueè¡¨ç¤ºå¯ç”¨è¿‡æ»¤ï¼ŒFalseè¡¨ç¤ºä¸è¿‡æ»¤ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤
    """
    if value is None:
        return None
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        value_lower = value.strip().lower()
        if value_lower == 'none' or value_lower == 'false':
            return None
        if value_lower == 'true':
            return True
    return None

def check_stock_info_filter(stock_name, stock_info_filter_config):
    """æ£€æŸ¥è‚¡ç¥¨æ˜¯å¦é€šè¿‡åŸºç¡€ä¿¡æ¯è¿‡æ»¤
    
    Args:
        stock_name: è‚¡ç¥¨åç§°
        stock_info_filter_config: è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤é…ç½®
    
    Returns:
        tuple: (æ˜¯å¦é€šè¿‡, è¿‡æ»¤åŸå› )
    """
    if not stock_info_filter_config.get('active', False):
        return True, None
    
    # 1. STè‚¡ç¥¨è¿‡æ»¤
    exclude_st = parse_bool_or_none(stock_info_filter_config.get('exclude_st', 'none'))
    if exclude_st:
        if 'ST' in stock_name or 'st' in stock_name.lower():
            return False, "STè‚¡ç¥¨"
    
    # æ³¨æ„ï¼šå¸‚å€¼å’Œå¸‚ç›ˆç‡éœ€è¦ä»å…¶ä»–æ•°æ®æºè·å–ï¼Œè¿™é‡Œæš‚æ—¶åªå®ç°STè¿‡æ»¤
    # å¸‚å€¼å’Œå¸‚ç›ˆç‡è¿‡æ»¤éœ€è¦åœ¨æœ‰stock_infoæ•°æ®çš„åœ°æ–¹å®ç°
    
    return True, None

def get_stock_info_from_csv(stock_code, stock_name, stock_info_filter_config):
    """ä»appendix.jsonè·å–è‚¡ç¥¨ä¿¡æ¯å¹¶è¿›è¡Œè¿‡æ»¤
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°
        stock_info_filter_config: è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤é…ç½®
    
    Returns:
        tuple: (æ˜¯å¦é€šè¿‡, è¿‡æ»¤åŸå› , è‚¡ç¥¨ä¿¡æ¯dict)
    """
    if not stock_info_filter_config.get('active', False):
        return True, None, {}
    
    # 1. STè‚¡ç¥¨è¿‡æ»¤
    exclude_st = parse_bool_or_none(stock_info_filter_config.get('exclude_st', 'none'))
    if exclude_st:
        if 'ST' in stock_name or 'st' in stock_name.lower():
            return False, "STè‚¡ç¥¨", {}
    
    # 2. å°è¯•ä»appendix.jsonåŠ è½½è‚¡ç¥¨ä¿¡æ¯
    stock_info = {}
    try:
        if os.path.exists('appendix.json'):
            with open('appendix.json', 'r', encoding='utf-8') as f:
                appendix_data = json.load(f)
                stock_info = appendix_data.get(stock_code, {})
    except Exception as e:
        logger.debug(f"æ— æ³•åŠ è½½appendix.json: {e}")
    
    # 3. å¸‚å€¼è¿‡æ»¤
    market_cap_range = parse_number_range(stock_info_filter_config.get('market_cap_range', 'none'))
    if market_cap_range is not None:
        market_cap = stock_info.get('market_cap')  # å•ä½ï¼šäº¿å…ƒ
        if market_cap is not None:
            try:
                market_cap = float(market_cap)
                if not (market_cap_range[0] <= market_cap <= market_cap_range[1]):
                    return False, f"å¸‚å€¼{market_cap:.2f}äº¿ä¸åœ¨èŒƒå›´{market_cap_range[0]}-{market_cap_range[1]}äº¿", stock_info
            except (ValueError, TypeError):
                logger.debug(f"è‚¡ç¥¨ {stock_code} å¸‚å€¼æ•°æ®æ— æ•ˆ: {market_cap}")
        else:
            logger.debug(f"è‚¡ç¥¨ {stock_code} æ— å¸‚å€¼æ•°æ®")
    
    # 4. å¸‚ç›ˆç‡TTMæœ€å°å€¼è¿‡æ»¤ï¼ˆè¿‡æ»¤ä½å¸‚ç›ˆç‡è‚¡ç¥¨ï¼Œä¿ç•™é«˜å¸‚ç›ˆç‡å’ŒäºæŸè‚¡ç¥¨ï¼‰
    pe_ttm_min_str = stock_info_filter_config.get('pe_ttm_min', 'none')
    if pe_ttm_min_str is not None and isinstance(pe_ttm_min_str, str) and pe_ttm_min_str.strip().lower() != 'none':
        try:
            pe_ttm_min = float(pe_ttm_min_str)
            pe_ttm = stock_info.get('pe_ttm')  # å¸‚ç›ˆç‡TTM
            if pe_ttm is not None:
                try:
                    pe_ttm_val = float(pe_ttm)
                    # è¿‡æ»¤æ¡ä»¶: 0 < pe_ttm < pe_ttm_minï¼ˆä½ä¼°å€¼è‚¡ç¥¨è¢«è¿‡æ»¤ï¼‰
                    # ä¿ç•™æ¡ä»¶: pe_ttm >= pe_ttm_min æˆ– pe_ttm <= 0ï¼ˆäºæŸï¼‰
                    if 0 < pe_ttm_val < pe_ttm_min:
                        return False, f"å¸‚ç›ˆç‡TTM {pe_ttm_val:.2f}ä½äºæœ€å°å€¼{pe_ttm_min}", stock_info
                except (ValueError, TypeError):
                    logger.debug(f"è‚¡ç¥¨ {stock_code} å¸‚ç›ˆç‡TTMæ•°æ®æ— æ•ˆ: {pe_ttm}")
                    # æ•°æ®æ— æ•ˆæ—¶é€šè¿‡ï¼ˆä¿å®ˆå¤„ç†ï¼‰
            else:
                logger.debug(f"è‚¡ç¥¨ {stock_code} æ— å¸‚ç›ˆç‡TTMæ•°æ®")
                # æ— æ•°æ®æ—¶é€šè¿‡ï¼ˆä¿å®ˆå¤„ç†ï¼‰
        except (ValueError, TypeError):
            logger.warning(f"å¸‚ç›ˆç‡TTMæœ€å°å€¼é…ç½®æ— æ•ˆ: {pe_ttm_min_str}")
    
    return True, None, stock_info

def find_kline_file(stock_code):
    """æŸ¥æ‰¾è‚¡ç¥¨å¯¹åº”çš„Kçº¿æ•°æ®æ–‡ä»¶"""
    # å°è¯•ä¸åŒçš„æ–‡ä»¶åæ ¼å¼
    possible_files = [
        f"data/{stock_code}.csv",
        f"data/{stock_code}_kline.csv",
        f"data/sz_{stock_code}_kline.csv",
        f"data/sh_{stock_code}_kline.csv"
    ]
    
    for file_path in possible_files:
        if os.path.exists(file_path):
            return file_path
    
    logger.warning(f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„Kçº¿æ•°æ®æ–‡ä»¶")
    return None

def load_kline_data(file_path):
    """åŠ è½½Kçº¿æ•°æ®"""
    try:
        df = pd.read_csv(file_path)
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date')
        return df
    except Exception as e:
        logger.error(f"åŠ è½½Kçº¿æ•°æ®å¤±è´¥ {file_path}: {e}")
        return None

def calculate_pre_high_metrics(kline_df, signal_date, pre_high_days=10):
    """
    è®¡ç®—å‰é«˜ä»·æ ¼ç›¸å…³æŒ‡æ ‡
    
    åŠŸèƒ½è¯´æ˜ï¼š
    1. è®¡ç®—ä¸Šç©¿å‰Nä¸ªäº¤æ˜“æ—¥çš„å¼€ç›˜ä»·å’Œæ”¶ç›˜ä»·ä¸­çš„æœ€é«˜ä»·M
    2. è®¡ç®—ä¸Šç©¿åæœ€é«˜æ”¶ç›˜ä»·Hå‘ç”Ÿæ—¥å¾€åçš„æœ€ä½æ”¶ç›˜ä»·Jç›¸å¯¹äºMçš„æ¶¨å¹…Kï¼ˆå‰é«˜æœ€å°æ¶¨å¹…ï¼‰
    3. è®¡ç®—å‰æœ€é«˜ä»·Måˆ°å½“å‰æ”¶ç›˜ä»·çš„æ¶¨å¹…Aï¼ˆå‰é«˜å½“å‰æ¶¨å¹…ï¼‰
    4. è®¡ç®—å‰æœ€é«˜ä»·Måˆ°å‘ç”Ÿä¸Šç©¿åæœ€é«˜æ”¶ç›˜ä»·Hçš„æ¶¨å¹…Bï¼ˆå‰é«˜æœ€å¤§æ¶¨å¹…ï¼‰
    
    Args:
        kline_df: Kçº¿æ•°æ®DataFrameï¼ŒåŒ…å«date, open, high, low, closeåˆ—
        signal_date: ä¸Šç©¿ä¿¡å·æ—¥æœŸ
        pre_high_days: ä¸Šç©¿å‰æŸ¥çœ‹çš„äº¤æ˜“æ—¥æ•°ï¼Œé»˜è®¤10å¤©
    
    Returns:
        dict: åŒ…å«ä»¥ä¸‹é”®å€¼å¯¹çš„å­—å…¸
            - pre_high_price: å‰é«˜ä»·æ ¼M
            - pre_high_min_range: å‰é«˜æœ€å°æ¶¨å¹…Kï¼ˆå¯èƒ½ä¸ºè´Ÿå€¼ï¼Œè¡¨ç¤ºè·Œå¹…ï¼‰
            - pre_high_current_range: å‰é«˜å½“å‰æ¶¨å¹…A
            - pre_high_max_range: å‰é«˜æœ€å¤§æ¶¨å¹…B
            å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›None
    """
    try:
        signal_date = pd.to_datetime(signal_date)
        
        # 1. è®¡ç®—ä¸Šç©¿å‰Nä¸ªäº¤æ˜“æ—¥çš„å¼€ç›˜ä»·å’Œæ”¶ç›˜ä»·ä¸­çš„æœ€é«˜ä»·M
        # æ‰¾åˆ°ä¿¡å·æ—¥æœŸåœ¨Kçº¿æ•°æ®ä¸­çš„ä½ç½®
        signal_idx = kline_df[kline_df['date'] == signal_date].index
        if len(signal_idx) == 0:
            logger.warning(f"æœªæ‰¾åˆ°ä¿¡å·æ—¥æœŸ {signal_date} çš„Kçº¿æ•°æ®")
            return None
        
        signal_idx = signal_idx[0]
        # è·å–ä¿¡å·æ—¥æœŸå‰Nä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
        start_idx = max(0, signal_idx - pre_high_days)
        pre_signal_data = kline_df.iloc[start_idx:signal_idx]
        
        if pre_signal_data.empty:
            logger.warning(f"åœ¨ä¸Šç©¿å‰{pre_high_days}ä¸ªäº¤æ˜“æ—¥å†…æœªæ‰¾åˆ°Kçº¿æ•°æ®")
            return None
        
        # è®¡ç®—å¼€ç›˜ä»·å’Œæ”¶ç›˜ä»·ä¸­çš„æœ€é«˜ä»·
        open_max = pre_signal_data['open'].max()
        close_max = pre_signal_data['close'].max()
        pre_high_price = max(open_max, close_max)  # M
        
        # 2. è·å–ä¸Šç©¿åçš„æ•°æ®
        post_signal_data = kline_df[kline_df['date'] > signal_date]
        
        # 5. è®¡ç®—å½“å‰æ”¶ç›˜ä»·ï¼ˆæœ€åä¸€å¤©çš„æ”¶ç›˜ä»·ï¼‰
        current_price = kline_df['close'].iloc[-1]
        
        if post_signal_data.empty:
            # å½“ä¸Šç©¿æ—¥æœŸä¸ºæœ€åä¸€å¤©æ—¶ï¼Œç›´æ¥ç”¨æœ€åä¸€å¤©æ”¶ç›˜ä»·è®¡ç®—æ¶¨è·Œå¹…
            logger.info(f"ä¸Šç©¿æ—¥æœŸ{signal_date}ä¸ºæœ€åä¸€å¤©ï¼Œç›´æ¥ä½¿ç”¨æœ€åä¸€å¤©æ”¶ç›˜ä»·è®¡ç®—æ¶¨è·Œå¹…")
            post_high_price = current_price  # H = å½“å‰æ”¶ç›˜ä»·
            post_min_price = current_price   # J = å½“å‰æ”¶ç›˜ä»·
        else:
            # 3. è®¡ç®—ä¸Šç©¿åæœ€é«˜æ”¶ç›˜ä»·H
            post_high_price = post_signal_data['close'].max()  # H
            post_high_date = post_signal_data[post_signal_data['close'] == post_high_price]['date'].iloc[0]
            
            # 4. è®¡ç®—ä¸Šç©¿åæœ€é«˜æ”¶ç›˜ä»·Hå‘ç”Ÿæ—¥å¾€åçš„æœ€ä½æ”¶ç›˜ä»·J
            post_high_data = kline_df[kline_df['date'] >= post_high_date]
            if post_high_data.empty:
                post_min_price = post_high_price  # å¦‚æœæ²¡æœ‰åç»­æ•°æ®ï¼Œä½¿ç”¨æœ€é«˜ä»·æœ¬èº«
            else:
                post_min_price = post_high_data['close'].min()  # J
        
        # 6. è®¡ç®—å„é¡¹æ¶¨å¹…
        if pre_high_price > 0:
            # å‰é«˜æœ€å°æ¶¨å¹…Kï¼šJç›¸å¯¹äºMçš„æ¶¨å¹…ï¼ˆå¯èƒ½ä¸ºè´Ÿå€¼ï¼‰
            pre_high_min_range = (post_min_price - pre_high_price) / pre_high_price
            
            # å‰é«˜å½“å‰æ¶¨å¹…Aï¼šå½“å‰æ”¶ç›˜ä»·ç›¸å¯¹äºMçš„æ¶¨å¹…
            pre_high_current_range = (current_price - pre_high_price) / pre_high_price
            
            # å‰é«˜æœ€å¤§æ¶¨å¹…Bï¼šä¸Šç©¿åæœ€é«˜æ”¶ç›˜ä»·Hç›¸å¯¹äºMçš„æ¶¨å¹…
            pre_high_max_range = (post_high_price - pre_high_price) / pre_high_price
            
            return {
                'pre_high_price': pre_high_price,
                'pre_high_min_range': pre_high_min_range,
                'pre_high_current_range': pre_high_current_range,
                'pre_high_max_range': pre_high_max_range
            }
        else:
            logger.warning(f"å‰é«˜ä»·æ ¼ä¸º0æˆ–è´Ÿæ•°ï¼Œæ— æ³•è®¡ç®—æ¶¨å¹…")
            return None
            
    except Exception as e:
        logger.error(f"è®¡ç®—å‰é«˜ä»·æ ¼ç›¸å…³æŒ‡æ ‡å¤±è´¥: {e}")
        return None

def calculate_max_range_during_crossover(kline_df, signal_date, lookback_days=3):
    """
    è®¡ç®—ä¸Šç©¿å‰ålookback_daysä¸ªäº¤æ˜“æ—¥å†…çš„æœ€å¤§æ¶¨å¹…
    
    Args:
        kline_df: Kçº¿æ•°æ®DataFrame
        signal_date: ä¸Šç©¿ä¿¡å·æ—¥æœŸ
        lookback_days: ä¸Šç©¿å‰åæŸ¥çœ‹çš„äº¤æ˜“æ—¥æ•°
    
    Returns:
        float: æœ€å¤§æ¶¨å¹…ç™¾åˆ†æ¯”ï¼Œè®¡ç®—å¤±è´¥æ—¶è¿”å›None
    """
    try:
        signal_date = pd.to_datetime(signal_date)
        
        # æ‰¾åˆ°ä¿¡å·æ—¥æœŸåœ¨Kçº¿æ•°æ®ä¸­çš„ä½ç½®
        signal_idx = kline_df[kline_df['date'] == signal_date].index
        if len(signal_idx) == 0:
            logger.warning(f"æœªæ‰¾åˆ°ä¿¡å·æ—¥æœŸ {signal_date} çš„Kçº¿æ•°æ®")
            return None
        
        signal_idx = signal_idx[0]
        
        # è®¡ç®—æŸ¥çœ‹èŒƒå›´ï¼šä¸Šç©¿å‰ålookback_daysä¸ªäº¤æ˜“æ—¥
        start_idx = max(0, signal_idx - lookback_days)
        end_idx = min(len(kline_df) - 1, signal_idx + lookback_days)
        
        # è·å–æŒ‡å®šäº¤æ˜“æ—¥èŒƒå›´å†…çš„æ•°æ®
        range_data = kline_df.iloc[start_idx:end_idx + 1]
        
        if range_data.empty:
            logger.warning(f"åœ¨ä¸Šç©¿å‰å{lookback_days}ä¸ªäº¤æ˜“æ—¥å†…æœªæ‰¾åˆ°Kçº¿æ•°æ®")
            return None
        
        # æ‰¾åˆ°æœŸé—´å†…çš„æœ€é«˜æ”¶ç›˜ä»·å’Œæœ€ä½å¼€ç›˜ä»·
        max_close = range_data['close'].max()
        min_open = range_data['open'].min()
        
        # è®¡ç®—æœ€å¤§æ¶¨å¹…
        if min_open > 0:
            max_range_up = (max_close - min_open) / min_open
            return max_range_up
        else:
            logger.warning(f"æœ€ä½å¼€ç›˜ä»·ä¸º0æˆ–è´Ÿæ•°ï¼Œæ— æ³•è®¡ç®—æ¶¨å¹…")
            return None
            
    except Exception as e:
        logger.error(f"è®¡ç®—ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…å¤±è´¥: {e}")
        return None

def calculate_price_change_after_signal(kline_df, signal_date):
    """
    è®¡ç®—ADXä¸Šç©¿ä¿¡å·æ—¥æœŸåçš„æœ€å¤§æ¶¨è·Œå¹…
    æ–°ç®—æ³•ï¼šä»¥ä¿¡å·æ—¥æœŸå½“å¤©çš„(å¼€ç›˜ä»·+æ”¶ç›˜ä»·)/2ä¸ºåŸºå‡†ï¼Œè®¡ç®—åˆ°Kçº¿æ•°æ®æœ€åä¸€å¤©çš„æœ€å¤§æ¶¨è·Œå¹…
    
    Args:
        kline_df: Kçº¿æ•°æ®DataFrame
        signal_date: ADXä¸Šç©¿ä¿¡å·æ—¥æœŸ
    
    Returns:
        tuple: (max_up_pct, max_down_pct, base_price, signal_date) æœ€å¤§æ¶¨å¹…ã€æœ€å¤§è·Œå¹…ã€æ¢è½´ç‚¹ä»·æ ¼ã€ä¿¡å·æ—¥æœŸ
    """
    try:
        signal_date = pd.to_datetime(signal_date)
        
        # æ‰¾åˆ°ä¿¡å·æ—¥æœŸå½“å¤©çš„æ•°æ®
        signal_day_data = kline_df[kline_df['date'] == signal_date]
        
        if signal_day_data.empty:
            logger.warning(f"æœªæ‰¾åˆ°ä¿¡å·æ—¥æœŸ {signal_date} çš„Kçº¿æ•°æ®")
            return None, None, None, None
        
        signal_row = signal_day_data.iloc[0]
        # ä½¿ç”¨ä¿¡å·æ—¥æœŸå½“å¤©çš„(æœ€é«˜ä»·+æœ€ä½ä»·+2*æ”¶ç›˜ä»·)/4ä½œä¸ºæ¢è½´ç‚¹ä»·æ ¼
        # base_price = (signal_row['high'] + signal_row['low'] + 2 * signal_row['close']) / 4
      
        #æˆ–è€…ä½¿ç”¨ä¿¡å·æ—¥æœŸå½“å¤©çš„æ”¶ç›˜ä»·ä½œä¸ºæ¢è½´ç‚¹ä»·æ ¼
        base_price = signal_row['close']
        
        # æˆ–è€…ä½¿ç”¨ä¿¡å·æ—¥æœŸå½“å¤©çš„å¼€ç›˜ä»·ä½œä¸ºæ¢è½´ç‚¹ä»·æ ¼
        # base_price = signal_row['open']


        # è·å–ä¿¡å·æ—¥æœŸåçš„æ‰€æœ‰æ•°æ®ï¼ˆä¸åŒ…å«ä¿¡å·æ—¥å½“å¤©ï¼‰
        future_data = kline_df[kline_df['date'] > signal_date]
        
        if future_data.empty:
            logger.warning(f"ä¿¡å·æ—¥æœŸ {signal_date} åæ²¡æœ‰Kçº¿æ•°æ®")
            logger.info(f"ä¸Šç©¿æ—¥æœŸ{signal_date}ä¸ºæœ€åä¸€å¤©ï¼Œç›´æ¥ä½¿ç”¨æœ€åä¸€å¤©æ”¶ç›˜ä»·è®¡ç®—æ¢è½´ç‚¹æ¶¨è·Œå¹…")
            # å½“ä¸Šç©¿æ—¥æœŸä¸ºæœ€åä¸€å¤©æ—¶ï¼Œå°†æœ€å¤§æœ€å°ä»·æ ¼éƒ½è®¾ç½®ä¸ºå½“å¤©æ”¶ç›˜ä»·
            current_close = signal_row['close']
            max_high = current_close
            min_low = current_close
        else:
            # è®¡ç®—æœŸé—´å†…çš„æ”¶ç›˜æœ€é«˜ä»·å’Œæœ€ä½ä»·
            max_high = future_data['close'].max()
            min_low = future_data['close'].min()
        
        # è®¡ç®—ç›¸å¯¹äºæ¢è½´ç‚¹ä»·æ ¼çš„æ¶¨è·Œå¹…
        max_up_pct = (max_high - base_price) / base_price
        max_down_pct = (min_low - base_price) / base_price  # ä¿®æ”¹ä¸ºè´Ÿå€¼è¡¨ç¤ºè·Œå¹…
        
        logger.debug(f"è‚¡ç¥¨ä¿¡å·æ—¥æœŸ: {signal_date}, æ¢è½´ç‚¹ä»·æ ¼: {base_price:.2f} (å¼€ç›˜:{signal_row['open']:.2f}, æ”¶ç›˜:{signal_row['close']:.2f}), "
                    f"æœŸé—´æœ€é«˜: {max_high:.2f}, æœŸé—´æœ€ä½: {min_low:.2f}, "
                    f"æœ€å¤§æ¶¨å¹…: {max_up_pct:.2%}, æœ€å¤§è·Œå¹…: {max_down_pct:.2%}")
        
        return max_up_pct, max_down_pct, base_price, signal_date
        
    except Exception as e:
        logger.error(f"è®¡ç®—ä»·æ ¼å˜åŒ–å¤±è´¥: {e}")
        return None, None, None, None

def calculate_current_range_from_base(kline_df, signal_date):
    """
    è®¡ç®—æ¢è½´ç‚¹ä»·æ ¼åˆ°æœ€åä¸€æ—¥çš„å½“å‰æ¶¨å¹…
    
    Args:
        kline_df: Kçº¿æ•°æ®DataFrame
        signal_date: ADXä¸Šç©¿ä¿¡å·æ—¥æœŸ
    
    Returns:
        float: å½“å‰æ¶¨å¹…ï¼ˆæ¢è½´ç‚¹ä»·æ ¼åˆ°æœ€åä¸€æ—¥æ”¶ç›˜ä»·çš„æ¶¨å¹…ï¼‰
    """
    try:
        signal_date = pd.to_datetime(signal_date)
        
        # æ‰¾åˆ°ä¿¡å·æ—¥æœŸå½“å¤©çš„æ•°æ®
        signal_day_data = kline_df[kline_df['date'] == signal_date]
        
        if signal_day_data.empty:
            logger.warning(f"æœªæ‰¾åˆ°ä¿¡å·æ—¥æœŸ {signal_date} çš„Kçº¿æ•°æ®")
            return 0.0
        
        # ä½¿ç”¨ä¿¡å·æ—¥æœŸå½“å¤©çš„(æœ€é«˜ä»·+æœ€ä½ä»·+2*æ”¶ç›˜ä»·)/4ä½œä¸ºæ¢è½´ç‚¹ä»·æ ¼
        signal_row = signal_day_data.iloc[0]
        base_price = (signal_row['high'] + signal_row['low'] + 2 * signal_row['close']) / 4
        
        # æ£€æŸ¥ä¿¡å·æ—¥æœŸæ˜¯å¦ä¸ºæœ€åä¸€å¤©
        future_data = kline_df[kline_df['date'] > signal_date]
        
        if future_data.empty:
            # å½“ä¸Šç©¿æ—¥æœŸä¸ºæœ€åä¸€å¤©æ—¶ï¼Œç›´æ¥ä½¿ç”¨å½“å¤©æ”¶ç›˜ä»·ä½œä¸ºå½“å‰ä»·æ ¼
            logger.info(f"ä¸Šç©¿æ—¥æœŸ{signal_date}ä¸ºæœ€åä¸€å¤©ï¼Œç›´æ¥ä½¿ç”¨å½“å¤©æ”¶ç›˜ä»·è®¡ç®—å½“å‰æ¶¨å¹…")
            current_price = signal_row['close']
        else:
            # è·å–æœ€åä¸€æ—¥çš„æ”¶ç›˜ä»·
            current_price = kline_df.iloc[-1]['close']
        
        # è®¡ç®—å½“å‰æ¶¨å¹…
        current_range = (current_price - base_price) / base_price
        
        return current_range
        
    except Exception as e:
        logger.error(f"è®¡ç®—å½“å‰æ¶¨å¹…æ—¶å‡ºé”™: {e}")
        return 0.0


def calculate_hislow_point_range(kline_df: pd.DataFrame, lookback_months: int | None = None) -> float | None:
    """
    è®¡ç®—æœ€åä¸€å¤©æ”¶ç›˜ä»·ç›¸å¯¹äºå†å²æœ€ä½ä»·çš„æ¶¨å¹…ã€‚

    - å½“æä¾› lookback_months æ—¶ï¼Œä»…ç»Ÿè®¡æœ€åæ—¥æœŸå¾€å‰ N æœˆå†…çš„æœ€ä½ä»·ã€‚
    - è¿”å›æ¯”ä¾‹å€¼ï¼Œä¾‹å¦‚ 0.16 è¡¨ç¤º 16%ã€‚è‹¥å†å²æœ€ä½ä»·<=0æˆ–æ•°æ®ç¼ºå¤±åˆ™è¿”å› Noneã€‚
    """
    try:
        if kline_df is None or kline_df.empty:
            return None
        # ç¡®ä¿æ—¥æœŸä¸º datetime
        dates = pd.to_datetime(kline_df['date'])
        last_close = float(kline_df['close'].iloc[-1])
        if lookback_months is not None and lookback_months > 0:
            last_date = dates.iloc[-1]
            start_date = last_date - pd.DateOffset(months=int(lookback_months))
            window_df = kline_df[dates >= start_date]
        else:
            window_df = kline_df
        if window_df is None or window_df.empty:
            logger.warning("æŒ‡å®šçª—å£å†…æ— æ•°æ®ï¼Œæ— æ³•è®¡ç®—å†å²ä½ç‚¹æ¶¨å¹…")
            return None
        hist_low = float(window_df['low'].min())
        if hist_low <= 0:
            logger.warning("å†å²æœ€ä½ä»·<=0ï¼Œæ— æ³•è®¡ç®—å†å²ä½ç‚¹æ¶¨å¹…")
            return None
        return (last_close - hist_low) / hist_low
    except Exception as e:
        logger.error(f"è®¡ç®—å†å²ä½ç‚¹æ¶¨å¹…å¤±è´¥: {e}")
        return None


def filter_results(input_file, output_file, range_up, range_down, file_type, lookback_days=3, max_range_up=0, pre_high_days=10, pre_high_min_range=0, pre_high_max_range=0, hislow_range: tuple | None = None, hislow_lookback_months: int | None = None, target_date=None, stock_info_filter_config=None):
    """
    è¿‡æ»¤ç»“æœæ–‡ä»¶
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        range_up: æœ€å¤§æ¶¨å¹…é˜ˆå€¼ï¼ˆå¯ä»¥ä¸ºè´Ÿå€¼è¡¨ç¤ºè·Œå¹…ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
        range_down: æœ€å°æ¶¨å¹…é˜ˆå€¼ï¼ˆå¯ä»¥ä¸ºè´Ÿå€¼è¡¨ç¤ºè·Œå¹…ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
        file_type: æ–‡ä»¶ç±»å‹ ('ADX' æˆ– 'PDI')
        lookback_days: ä¸Šç©¿å‰åæŸ¥çœ‹çš„å¤©æ•°
        max_range_up: ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…é˜ˆå€¼ï¼ˆNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
        pre_high_days: å‰é«˜å¤©æ•°N
        pre_high_min_range: å‰é«˜æœ€å°æ¶¨å¹…é˜ˆå€¼Kï¼ˆå¯ä»¥ä¸ºè´Ÿå€¼è¡¨ç¤ºè·Œå¹…ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
        pre_high_max_range: å‰é«˜æœ€å¤§æ¶¨å¹…é˜ˆå€¼ï¼ˆå‰é«˜ä»·æ ¼Måˆ°ä¸Šç©¿åæœ€é«˜æ”¶ç›˜ä»·Hçš„æ¶¨å¹…ï¼Œå¯ä»¥ä¸ºè´Ÿå€¼è¡¨ç¤ºè·Œå¹…ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
        stock_info_filter_config: è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤é…ç½®
    """
    if stock_info_filter_config is None:
        stock_info_filter_config = {}
    try:
        # è¯»å–ç»“æœæ–‡ä»¶
        df = pd.read_csv(input_file)
        file_type = "PDI" if "PDI" in os.path.basename(input_file) else "ADX"
        logger.info(f"è¯»å–{file_type}ç»“æœæ–‡ä»¶: {input_file}, å…± {len(df)} æ¡è®°å½•")
        
        filtered_results = []
        
        for idx, row in df.iterrows():
            original_code = str(row['code'])
            stock_name = str(row.get('name', ''))
            signal_date = row['date']
            
            # è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤
            stock_info_passed, filter_reason, stock_info = get_stock_info_from_csv(
                original_code, stock_name, stock_info_filter_config
            )
            if not stock_info_passed:
                logger.info(f"è‚¡ç¥¨ {original_code} {stock_name} è¢«è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤: {filter_reason}")
                continue
            
            # å¦‚æœæŒ‡å®šäº†target_dateï¼Œåˆ™ä½¿ç”¨target_dateæ›¿æ¢åŸå§‹æ—¥æœŸ
            if target_date:
                signal_date = target_date
                logger.debug(f"è‚¡ç¥¨ {original_code} æ—¥æœŸä» {row['date']} ä¿®æ­£ä¸º {target_date}")
            
            # ç›´æ¥ä»dataæ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾åŒ¹é…çš„CSVæ–‡ä»¶
            data_dir = 'data'
            matched_code = None
            kline_file = None
            
            # å°è¯•ä¸åŒçš„ä»£ç æ ¼å¼æ¥åŒ¹é…dataæ–‡ä»¶å¤¹ä¸­çš„CSVæ–‡ä»¶
            possible_codes = [
                original_code,  # åŸå§‹ä»£ç 
                original_code.zfill(6),  # 6ä½è¡¥é›¶
                original_code.lstrip('0') if len(original_code) > 1 else original_code  # å»æ‰å‰å¯¼é›¶
            ]
            
            for code in possible_codes:
                csv_file = os.path.join(data_dir, f"{code}.csv")
                if os.path.exists(csv_file):
                    matched_code = code
                    kline_file = csv_file
                    break
            
            if not kline_file:
                logger.debug(f"æœªæ‰¾åˆ°è‚¡ç¥¨ {original_code} çš„Kçº¿æ•°æ®æ–‡ä»¶")
                continue
            
            # åŠ è½½Kçº¿æ•°æ®
            kline_df = load_kline_data(kline_file)
            if kline_df is None:
                continue
            
            # è®¡ç®—ä»·æ ¼å˜åŒ–
            max_up_pct, max_down_pct, base_price, signal_date_used = calculate_price_change_after_signal(kline_df, signal_date)
            if max_up_pct is None or max_down_pct is None or base_price is None:
                continue
            
            # è®¡ç®—ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…
            crossover_max_range = calculate_max_range_during_crossover(kline_df, signal_date, lookback_days)
            if crossover_max_range is None:
                logger.warning(f"è‚¡ç¥¨ {matched_code} æ— æ³•è®¡ç®—ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…")
                continue
            
            # è®¡ç®—å‰é«˜ä»·æ ¼ç›¸å…³æŒ‡æ ‡
            pre_high_metrics = calculate_pre_high_metrics(kline_df, signal_date, pre_high_days)
            if pre_high_metrics is None:
                logger.warning(f"è‚¡ç¥¨ {matched_code} æ— æ³•è®¡ç®—å‰é«˜ä»·æ ¼ç›¸å…³æŒ‡æ ‡")
                continue

            # è®¡ç®—å†å²ä½ç‚¹æ¶¨å¹…ï¼ˆæŒ‰é…ç½®çš„ lookback_months çª—å£ï¼‰
            hislow_pct = calculate_hislow_point_range(kline_df, hislow_lookback_months)
            # å†å²ä½ç‚¹æ¶¨å¹…è¿‡æ»¤ï¼ˆNone è¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
            hislow_ok = hislow_range is None or (hislow_pct is not None and hislow_range[0] <= hislow_pct <= hislow_range[1])
            
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆè¿‡æ»¤æ¡ä»¶
            # è®¡ç®—å½“å‰æ¶¨å¹…ï¼ˆæ¢è½´ç‚¹ä»·æ ¼åˆ°æœ€åä¸€æ—¥çš„æ¶¨å¹…ï¼‰
            current_range = calculate_current_range_from_base(kline_df, signal_date)
            
            # 1. åŸæœ‰çš„æ¶¨è·Œå¹…è¿‡æ»¤æ¡ä»¶ï¼ˆrange_upä¸ºæœ€å¤§æ¶¨å¹…é™åˆ¶ï¼Œrange_downä¸ºæœ€å°æ¶¨å¹…è¦æ±‚ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
            up_ok = range_up is None or max_up_pct <= range_up
            # minRangeé€»è¾‘ï¼šå½“ä¸ºæ­£å€¼æ—¶è¡¨ç¤ºæœ€å°æ¶¨å¹…è¦æ±‚ï¼Œå½“ä¸ºè´Ÿå€¼æ—¶è¡¨ç¤ºæœ€å¤§è·Œå¹…é™åˆ¶
            # ä¾‹å¦‚ï¼šminRange=0%è¡¨ç¤ºä¸å…è®¸è·Œå¹…ï¼ŒminRange=-1%è¡¨ç¤ºæœ€å¤§è·Œå¹…ä¸è¶…è¿‡-1%
            down_ok = range_down is None or max_down_pct >= range_down
            
            # 2. æ–°å¢çš„ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…è¿‡æ»¤æ¡ä»¶ï¼ˆNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
            crossover_ok = max_range_up is None or crossover_max_range >= max_range_up
            
            # 3. æ–°å¢çš„å‰é«˜æœ€å°æ¶¨å¹…è¿‡æ»¤æ¡ä»¶ï¼ˆpreHighMinRangeé€»è¾‘åŒminRangeï¼‰
            # å½“ä¸ºæ­£å€¼æ—¶è¡¨ç¤ºæœ€å°æ¶¨å¹…è¦æ±‚ï¼Œå½“ä¸ºè´Ÿå€¼æ—¶è¡¨ç¤ºæœ€å¤§è·Œå¹…é™åˆ¶ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤
            # ä¾‹å¦‚ï¼špreHighMinRange=0%è¡¨ç¤ºä¸å…è®¸è·Œå¹…ï¼ŒpreHighMinRange=-1%è¡¨ç¤ºæœ€å¤§è·Œå¹…ä¸è¶…è¿‡-1%
            pre_high_ok = pre_high_min_range is None or pre_high_metrics['pre_high_min_range'] >= pre_high_min_range
            
            # 4. æ–°å¢çš„å‰é«˜æœ€å¤§æ¶¨å¹…è¿‡æ»¤æ¡ä»¶ï¼ˆpre_high_max_rangeå¯ä»¥ä¸ºè´Ÿå€¼è¡¨ç¤ºè·Œå¹…ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
            pre_high_max_ok = pre_high_max_range is None or pre_high_metrics['pre_high_max_range'] <= pre_high_max_range
            
            if up_ok and down_ok and crossover_ok and pre_high_ok and pre_high_max_ok and hislow_ok:
                # åˆ›å»ºæ–°çš„è¡Œæ•°æ®ï¼ŒåŒ…å«åŸæœ‰å­—æ®µå’Œæ–°å¢çš„å­—æ®µ
                new_row = row.copy()
                new_row['code'] = matched_code  # ä½¿ç”¨ä»dataæ–‡ä»¶å¤¹ä¸­åŒ¹é…åˆ°çš„ä»£ç 
                # å¦‚æœæŒ‡å®šäº†target_dateï¼Œåˆ™æ›´æ–°æ—¥æœŸå­—æ®µ
                if target_date:
                    new_row['date'] = target_date
                # å†å²ä½ç‚¹æ¶¨å¹…ï¼ˆåœ¨CSVä¸­ä½äºæ¢è½´ç‚¹ä»·æ ¼ä¹‹å‰ï¼‰ï¼Œå¹¶æ³¨æ˜çª—å£æœˆæ•°
                hislow_label = f"å†å²ä½ç‚¹æ¶¨å¹…({hislow_lookback_months}æœˆ)" if hislow_lookback_months else "å†å²ä½ç‚¹æ¶¨å¹…"
                new_row[hislow_label] = f"{round((hislow_pct or 0) * 100, 2)}%"
                new_row['æ¢è½´ç‚¹ä»·æ ¼'] = round(base_price, 2)
                new_row['å½“å‰æ¶¨å¹…'] = f"{round(current_range * 100, 2)}%"  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å¹¶æ·»åŠ ç™¾åˆ†å·
                new_row['æœ€å°æ¶¨å¹…'] = f"{round(max_down_pct * 100, 2)}%"  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å¹¶æ·»åŠ ç™¾åˆ†å·
                new_row['æœ€å¤§æ¶¨å¹…'] = f"{round(max_up_pct * 100, 2)}%"  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å¹¶æ·»åŠ ç™¾åˆ†å·
                new_row['ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…'] = f"{round(crossover_max_range * 100, 2)}%"  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å¹¶æ·»åŠ ç™¾åˆ†å·
                new_row['å‰é«˜ä»·æ ¼'] = round(pre_high_metrics['pre_high_price'], 2)  # å‰é«˜ä»·æ ¼
                new_row['å‰é«˜å½“å‰æ¶¨å¹…'] = f"{round(pre_high_metrics['pre_high_current_range'] * 100, 2)}%"  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å¹¶æ·»åŠ ç™¾åˆ†å·
                new_row['å‰é«˜æœ€å°æ¶¨å¹…'] = f"{round(pre_high_metrics['pre_high_min_range'] * 100, 2)}%"  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å¹¶æ·»åŠ ç™¾åˆ†å·
                new_row['å‰é«˜æœ€å¤§æ¶¨å¹…'] = f"{round(pre_high_metrics['pre_high_max_range'] * 100, 2)}%"  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å¹¶æ·»åŠ ç™¾åˆ†å·
                
                filtered_results.append(new_row)
                logger.info(f"è‚¡ç¥¨ {matched_code} é€šè¿‡è¿‡æ»¤: ä¿¡å·æ—¥æœŸ {signal_date}, "
                           f"æ¢è½´ç‚¹ä»·æ ¼ {base_price:.2f}, å½“å‰æ¶¨å¹… {current_range:.2%}, æœ€å¤§æ¶¨å¹… {max_up_pct:.2%}, æœ€å°æ¶¨å¹… {max_down_pct:.2%}, "
                           f"ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹… {crossover_max_range:.2%}, "
                           f"å‰é«˜ä»·æ ¼ {pre_high_metrics['pre_high_price']:.2f}, "
                           f"å‰é«˜æœ€å°æ¶¨å¹… {pre_high_metrics['pre_high_min_range']:.2%}, "
                           f"å‰é«˜æœ€å¤§æ¶¨å¹… {pre_high_metrics['pre_high_max_range']:.2%}")
            else:
                logger.info(f"è‚¡ç¥¨ {matched_code} è¢«è¿‡æ»¤: ä¿¡å·æ—¥æœŸ {signal_date}, "
                           f"æ¢è½´ç‚¹ä»·æ ¼ {base_price:.2f}, å½“å‰æ¶¨å¹… {current_range:.2%}, æœ€å¤§æ¶¨å¹… {max_up_pct:.2%}, æœ€å°æ¶¨å¹… {max_down_pct:.2%}, "
                           f"ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹… {crossover_max_range:.2%}, "
                           f"å‰é«˜ä»·æ ¼ {pre_high_metrics['pre_high_price']:.2f}, "
                           f"å‰é«˜æœ€å°æ¶¨å¹… {pre_high_metrics['pre_high_min_range']:.2%}, "
                           f"å‰é«˜æœ€å¤§æ¶¨å¹… {pre_high_metrics['pre_high_max_range']:.2%}, "
                           f"å†å²ä½ç‚¹æ¶¨å¹…({hislow_lookback_months}æœˆ) {'' if hislow_pct is None else f'{hislow_pct:.2%}'} "
                           f"(æ¶¨å¹…è¶…é™: {max_up_pct > range_up if range_up is not None else False}, "
                           f"è·Œå¹…è¶…é™: {max_down_pct > range_down if range_down is not None else False}, "
                           f"ä¸Šç©¿æœŸé—´æ¶¨å¹…ä¸è¶³: {crossover_max_range < max_range_up if max_range_up is not None else False}, "
                           f"å‰é«˜æœ€å°æ¶¨å¹…ä¸è¶³: {pre_high_metrics['pre_high_min_range'] < pre_high_min_range if pre_high_min_range is not None else False}, "
                           f"å‰é«˜æœ€å¤§æ¶¨å¹…è¶…é™: {pre_high_metrics['pre_high_max_range'] > pre_high_max_range if pre_high_max_range is not None else False}, "
                           f"å†å²ä½ç‚¹æ¶¨å¹…ä¸åœ¨åŒºé—´: {False if hislow_range is None or hislow_pct is None else not (hislow_range[0] <= hislow_pct <= hislow_range[1])})")
        
        # ä¿å­˜è¿‡æ»¤ç»“æœ
        if filtered_results:
            filtered_df = pd.DataFrame(filtered_results)
            # ç»Ÿä¸€åˆ—é¡ºåºï¼Œç¡®ä¿â€œå†å²ä½ç‚¹æ¶¨å¹…(XXæœˆ)â€ä½äºâ€œæ¢è½´ç‚¹ä»·æ ¼â€ä¹‹å‰
            base_cols = list(df.columns)
            hislow_label = f"å†å²ä½ç‚¹æ¶¨å¹…({hislow_lookback_months}æœˆ)" if hislow_lookback_months else "å†å²ä½ç‚¹æ¶¨å¹…"
            extra_cols = [hislow_label, 'æ¢è½´ç‚¹ä»·æ ¼', 'å½“å‰æ¶¨å¹…', 'æœ€å°æ¶¨å¹…', 'æœ€å¤§æ¶¨å¹…', 'ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…', 'å‰é«˜ä»·æ ¼', 'å‰é«˜å½“å‰æ¶¨å¹…', 'å‰é«˜æœ€å°æ¶¨å¹…', 'å‰é«˜æœ€å¤§æ¶¨å¹…']
            desired_cols = base_cols + [c for c in extra_cols if c not in base_cols]
            filtered_df = filtered_df.reindex(columns=desired_cols)
            filtered_df.to_csv(output_file, index=False)
            logger.info(f"è¿‡æ»¤å®Œæˆï¼Œä¿å­˜ {len(filtered_results)} æ¡è®°å½•åˆ°: {output_file}")
        else:
            logger.warning("æ²¡æœ‰è‚¡ç¥¨é€šè¿‡è¿‡æ»¤æ¡ä»¶")
            # åˆ›å»ºç©ºæ–‡ä»¶ä½†ä¿æŒç›¸åŒçš„åˆ—ç»“æ„ï¼ŒåŒ…å«æ–°å¢çš„å­—æ®µ
            hislow_label = f"å†å²ä½ç‚¹æ¶¨å¹…({hislow_lookback_months}æœˆ)" if hislow_lookback_months else "å†å²ä½ç‚¹æ¶¨å¹…"
            columns = list(df.columns) + [hislow_label, 'æ¢è½´ç‚¹ä»·æ ¼', 'å½“å‰æ¶¨å¹…', 'æœ€å°æ¶¨å¹…', 'æœ€å¤§æ¶¨å¹…', 'ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…', 'å‰é«˜ä»·æ ¼', 'å‰é«˜å½“å‰æ¶¨å¹…', 'å‰é«˜æœ€å°æ¶¨å¹…', 'å‰é«˜æœ€å¤§æ¶¨å¹…']
            empty_df = pd.DataFrame(columns=columns)
            empty_df.to_csv(output_file, index=False)
            
    except Exception as e:
        logger.error(f"è¿‡æ»¤ç»“æœå¤±è´¥: {e}")

def process_single_file(input_file, output_dir, file_type, config, target_date=None, stock_info_filter_config=None):
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        file_type: æ–‡ä»¶ç±»å‹ ('ADX' æˆ– 'PDI')
        config: é…ç½®å­—å…¸
        target_date: ç›®æ ‡æ—¥æœŸï¼Œç”¨äºä¿®æ­£CSVæ–‡ä»¶ä¸­çš„æ—¥æœŸå­—æ®µ
        stock_info_filter_config: è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤é…ç½®
    
    Returns:
        str: å¤„ç†ç»“æœä¿¡æ¯
    """
    try:
        filename = os.path.basename(input_file)
        output_file = os.path.join(output_dir, filename)
        
        # è§£æé…ç½®å‚æ•°
        range_up = parse_percentage(config.get('maxRange', '0%'))
        range_down = parse_percentage(config.get('minRange', '0%'))
        lookback_days = config.get('lookback_days', 3)
        max_range_up = parse_percentage(config.get('maxRangeUp', '0%'))
        pre_high_days = config.get('preHighDays', 10)
        pre_high_min_range = parse_percentage(config.get('preHighMinRange', '0%'))
        pre_high_max_range = parse_percentage(config.get('preHighMaxRange', '0%'))
        hislow_range = parse_range_percent(config.get('HisLowPointRange', 'none'))
        lookback_months = config.get('lookback_months', 120)
        
        logger.info(f"å¼€å§‹å¤„ç†{file_type}æ–‡ä»¶: {input_file}")
        filter_results(input_file, output_file, range_up, range_down, file_type, 
                     lookback_days, max_range_up, pre_high_days, pre_high_min_range, 
                     pre_high_max_range, hislow_range, lookback_months, target_date, 
                     stock_info_filter_config)
        
        return f"æˆåŠŸå¤„ç† {filename}"
    except Exception as e:
        error_msg = f"å¤„ç†æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {str(e)}"
        logger.error(error_msg)
        return error_msg


def main():
    # ç”Ÿæˆå¸¦æ—¥æœŸçš„é»˜è®¤è¾“å‡ºç›®å½•
    current_date = datetime.now().strftime('%Y%m%d')
    default_output_dir = f'{current_date}-resByFilter'
    
    # ç”Ÿæˆå¸¦æ—¥æœŸçš„é»˜è®¤è¾“å…¥ç›®å½•
    default_input_dir = f'{current_date}-res'
    
    parser = argparse.ArgumentParser(description='ADXå’ŒPDIç»“æœè¿‡æ»¤å™¨')
    parser.add_argument('--input-dir', default=default_input_dir, help=f'è¾“å…¥ç›®å½• (é»˜è®¤: {default_input_dir})')
    parser.add_argument('--output-dir', default=default_output_dir, help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {default_output_dir})')
    parser.add_argument('--workers', type=int, default=12, help='å¹¶å‘å¤„ç†çš„çº¿ç¨‹æ•°')
    parser.add_argument('--date', help='æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œç”¨äºä¿®æ­£CSVæ–‡ä»¶ä¸­çš„æ—¥æœŸå­—æ®µå¹¶ç¡®å®šè¾“å…¥ç›®å½•')
    
    args = parser.parse_args()
    
    # è®°å½•ç”¨æˆ·æ˜¯å¦æ˜ç¡®æŒ‡å®šäº†è¾“å…¥ç›®å½•
    user_specified_input_dir = '--input-dir' in sys.argv
    
    # å¦‚æœæŒ‡å®šäº†æ—¥æœŸå‚æ•°ï¼Œå°†å…¶è½¬æ¢ä¸ºç›®æ ‡æ—¥æœŸæ ¼å¼å¹¶æ›´æ–°è¾“å…¥ç›®å½•
    target_date = None
    if args.date:
        try:
            # éªŒè¯æ—¥æœŸæ ¼å¼å¹¶è½¬æ¢
            date_obj = datetime.strptime(args.date, '%Y-%m-%d')
            target_date = args.date
            
            # åªæœ‰åœ¨ç”¨æˆ·æ²¡æœ‰æ˜ç¡®æŒ‡å®šè¾“å…¥ç›®å½•æ—¶ï¼Œæ‰æ ¹æ®æ—¥æœŸå‚æ•°è‡ªåŠ¨è®¾ç½®
            if not user_specified_input_dir:
                specified_date = date_obj.strftime('%Y%m%d')
                args.input_dir = f'{specified_date}-res'
                logger.info(f"æ ¹æ®æŒ‡å®šæ—¥æœŸ {target_date}ï¼Œè¾“å…¥ç›®å½•è®¾ç½®ä¸º: {args.input_dir}")
            
            logger.info(f"å°†ä½¿ç”¨æŒ‡å®šæ—¥æœŸ {target_date} ä¿®æ­£CSVæ–‡ä»¶ä¸­çš„æ—¥æœŸå­—æ®µ")
        except ValueError:
            logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {args.date}ï¼Œè¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼")
            return
    
    # åŠ è½½é…ç½®
    configs = load_config()
    adx_config = configs['ADX']
    pdi_config = configs['PDI']
    stock_info_filter_config = configs['stock_info_filter']
    
    # æ‰“å°è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤é…ç½®
    if stock_info_filter_config.get('active', False):
        logger.info("=" * 60)
        logger.info("è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤é…ç½®:")
        exclude_st = parse_bool_or_none(stock_info_filter_config.get('exclude_st', 'none'))
        market_cap_range = parse_number_range(stock_info_filter_config.get('market_cap_range', 'none'))
        
        # è§£æå¸‚ç›ˆç‡TTMæœ€å°å€¼
        pe_ttm_min_str = stock_info_filter_config.get('pe_ttm_min', 'none')
        pe_ttm_min = None
        if pe_ttm_min_str is not None and isinstance(pe_ttm_min_str, str) and pe_ttm_min_str.strip().lower() != 'none':
            try:
                pe_ttm_min = float(pe_ttm_min_str)
            except (ValueError, TypeError):
                pass
        
        logger.info(f"  - è¿‡æ»¤STè‚¡ç¥¨: {'æ˜¯' if exclude_st else 'å¦'}")
        logger.info(f"  - å¸‚å€¼èŒƒå›´: {'æ— é™åˆ¶' if market_cap_range is None else f'{market_cap_range[0]}-{market_cap_range[1]}äº¿å…ƒ'}")
        logger.info(f"  - å¸‚ç›ˆç‡TTMæœ€å°å€¼: {'æ— é™åˆ¶' if pe_ttm_min is None else f'{pe_ttm_min}å€ï¼ˆè¿‡æ»¤ä½äºæ­¤å€¼ï¼Œä¿ç•™é«˜äºæ­¤å€¼å’ŒäºæŸï¼‰'}")
        logger.info("=" * 60)
    else:
        logger.info("è‚¡ç¥¨ä¿¡æ¯è¿‡æ»¤æœªå¯ç”¨")
    
    # æ¸…ç©ºå¹¶é‡æ–°åˆ›å»ºè¾“å‡ºç›®å½•
    if os.path.exists(args.output_dir):
        import shutil
        logger.info(f"ğŸ—‘ï¸  æ¸…ç©ºè¾“å‡ºç›®å½•: {args.output_dir}")
        try:
            shutil.rmtree(args.output_dir)
            logger.info(f"âœ… å·²æ¸…ç©ºè¾“å‡ºç›®å½•")
        except Exception as e:
            logger.warning(f"âš ï¸  æ¸…ç©ºè¾“å‡ºç›®å½•æ—¶å‡ºé”™: {e}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    logger.info(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {args.output_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ADXå’ŒPDIç»“æœæ–‡ä»¶
    adx_files = glob.glob(os.path.join(args.input_dir, 'ADX*.csv'))
    pdi_files = glob.glob(os.path.join(args.input_dir, 'PDI*.csv'))
    
    if not adx_files and not pdi_files:
        logger.warning(f"åœ¨ {args.input_dir} ç›®å½•ä¸‹æœªæ‰¾åˆ°ADXæˆ–PDIç»“æœæ–‡ä»¶")
        return
    
    logger.info(f"æ‰¾åˆ° {len(adx_files)} ä¸ªADXç»“æœæ–‡ä»¶, {len(pdi_files)} ä¸ªPDIç»“æœæ–‡ä»¶")
    
    # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„ä»»åŠ¡åˆ—è¡¨
    tasks = []
    
    # å¤„ç†ADXæ–‡ä»¶
    if adx_files and adx_config.get('active', False):
        adx_range_up = parse_percentage(adx_config.get('maxRange', '0%'))
        adx_range_down = parse_percentage(adx_config.get('minRange', '0%'))
        adx_max_range_up = parse_percentage(adx_config.get('maxRangeUp', '0%'))
        adx_pre_high_min_range = parse_percentage(adx_config.get('preHighMinRange', '0%'))
        adx_pre_high_max_range = parse_percentage(adx_config.get('preHighMaxRange', '0%'))
        adx_hislow_range = parse_range_percent(adx_config.get('HisLowPointRange', 'none'))
        
        logger.info(f"ADXè¿‡æ»¤é…ç½®: æœ€å¤§æ¶¨å¹… {'none' if adx_range_up is None else f'{adx_range_up:.2%}'}, "
                   f"æœ€å¤§è·Œå¹… {'none' if adx_range_down is None else f'{adx_range_down:.2%}'}, "
                   f"å›çœ‹å¤©æ•° {adx_config.get('lookback_days', 3)}, ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…é˜ˆå€¼ {'none' if adx_max_range_up is None else f'{adx_max_range_up:.2%}'}, "
                   f"å‰é«˜å¤©æ•° {adx_config.get('preHighDays', 10)}, å‰é«˜æœ€å°æ¶¨å¹…é˜ˆå€¼ {'none' if adx_pre_high_min_range is None else f'{adx_pre_high_min_range:.2%}'}, "
                   f"å‰é«˜æœ€å¤§æ¶¨å¹…é˜ˆå€¼ {'none' if adx_pre_high_max_range is None else f'{adx_pre_high_max_range:.2%}'}, "
                   f"å†å²ä½ç‚¹æ¶¨å¹…åŒºé—´ {'none' if adx_hislow_range is None else f'{adx_hislow_range[0]:.2%}-{adx_hislow_range[1]:.2%}'}, "
                   f"å†å²ä½ç‚¹æ¶¨å¹…çª—å£ {adx_config.get('lookback_months', 120)}æœˆ")
        
        for input_file in adx_files:
            tasks.append((input_file, args.output_dir, 'ADX', adx_config, target_date, stock_info_filter_config))
    elif adx_files:
        logger.info("ADXé…ç½®æœªå¯ç”¨ï¼Œè·³è¿‡ADXæ–‡ä»¶å¤„ç†")
    
    # å¤„ç†PDIæ–‡ä»¶
    if pdi_files and pdi_config.get('active', False):
        pdi_range_up = parse_percentage(pdi_config.get('maxRange', '0%'))
        pdi_range_down = parse_percentage(pdi_config.get('minRange', '0%'))
        pdi_max_range_up = parse_percentage(pdi_config.get('maxRangeUp', '0%'))
        pdi_pre_high_min_range = parse_percentage(pdi_config.get('preHighMinRange', '0%'))
        pdi_pre_high_max_range = parse_percentage(pdi_config.get('preHighMaxRange', '0%'))
        pdi_hislow_range = parse_range_percent(pdi_config.get('HisLowPointRange', 'none'))
        
        logger.info(f"PDIè¿‡æ»¤é…ç½®: æœ€å¤§æ¶¨å¹… {'none' if pdi_range_up is None else f'{pdi_range_up:.2%}'}, "
                   f"æœ€å¤§è·Œå¹… {'none' if pdi_range_down is None else f'{pdi_range_down:.2%}'}, "
                   f"å›çœ‹å¤©æ•° {pdi_config.get('lookback_days', 3)}, ä¸Šç©¿æœŸé—´æœ€å¤§æ¶¨å¹…é˜ˆå€¼ {'none' if pdi_max_range_up is None else f'{pdi_max_range_up:.2%}'}, "
                   f"å‰é«˜å¤©æ•° {pdi_config.get('preHighDays', 10)}, å‰é«˜æœ€å°æ¶¨å¹…é˜ˆå€¼ {'none' if pdi_pre_high_min_range is None else f'{pdi_pre_high_min_range:.2%}'}, "
                   f"å‰é«˜æœ€å¤§æ¶¨å¹…é˜ˆå€¼ {'none' if pdi_pre_high_max_range is None else f'{pdi_pre_high_max_range:.2%}'}, "
                   f"å†å²ä½ç‚¹æ¶¨å¹…åŒºé—´ {'none' if pdi_hislow_range is None else f'{pdi_hislow_range[0]:.2%}-{pdi_hislow_range[1]:.2%}'}, "
                   f"å†å²ä½ç‚¹æ¶¨å¹…çª—å£ {pdi_config.get('lookback_months', 120)}æœˆ")
        
        for input_file in pdi_files:
            tasks.append((input_file, args.output_dir, 'PDI', pdi_config, target_date, stock_info_filter_config))
    elif pdi_files:
        logger.info("PDIé…ç½®æœªå¯ç”¨ï¼Œè·³è¿‡PDIæ–‡ä»¶å¤„ç†")
    
    if not tasks:
        logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶")
        return
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
    logger.info(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(tasks)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {args.workers} ä¸ªçº¿ç¨‹")
    
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [
            executor.submit(process_single_file, input_file, output_dir, file_type, config, target_date, stock_info_filter_config)
            for input_file, output_dir, file_type, config, target_date, stock_info_filter_config in tasks
        ]
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        for future in tqdm(as_completed(futures), total=len(futures), desc="å¤„ç†è¿›åº¦"):
            result = future.result()
            logger.debug(result)
    
    logger.info("æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ")

if __name__ == "__main__":
    main()